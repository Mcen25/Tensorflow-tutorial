{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.16.2)\n",
      "Collecting tensorflow-gpu\n",
      "  Using cached tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring version 2.12.0 of tensorflow-gpu since it has invalid metadata:\n",
      "Requested tensorflow-gpu from https://files.pythonhosted.org/packages/8a/45/fa31ced1db38f9424f262dfbf35747fe5378b5c808cecb373c8cb8e515d3/tensorflow-gpu-2.12.0.tar.gz has invalid metadata: Expected end or semicolon (after name and no valid version specifier)\n",
      "    python_version>\"3.7\"\n",
      "                  ^\n",
      "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-gpu (from versions: 2.12.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-gpu\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow tensorflow-gpu opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import imghdr as ihdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['05-12-21-happy-people.jpg',\n",
       " 'image22.jpeg',\n",
       " 'Happy.jpg',\n",
       " '7-principles-of-successful-and-happy-people.png',\n",
       " '1687d86811a2c25b877448ed47218131.jpg',\n",
       " '170404-happy-workers-feature.jpg',\n",
       " 'goup-happy-people-group-jumping-isolated-white-background-35582232.jpg',\n",
       " 'happy-family-happy-people.jpg',\n",
       " 'image14.jpeg',\n",
       " 'image15.jpeg',\n",
       " 'maxresdefault2.jpg',\n",
       " 'file-20230208-27-3jttof.jpg',\n",
       " 'habits-of-happy-people-jpg.jpg',\n",
       " '.DS_Store',\n",
       " 'friends_190412.jpg',\n",
       " 'aecd7b04-bb40-42ad-9985-d63a3198a878.jpg',\n",
       " 'getty_107808334_121413.jpg',\n",
       " '1920px-face-smile.svg_.png',\n",
       " 'gdfVVm_MyCRtqpvdkt8vtSB1n_oz_CpwCq6vNMpj0S8.jpg',\n",
       " '7-Habits-of-Happy-People.png',\n",
       " 'group-people-posing-photo-with-words-happy-bottom_577115-4097.jpg',\n",
       " 'image23.jpeg',\n",
       " 'es_27x40_pre_final_en-us_cps_custom-3e4a47095475f6fd5a6da815e62e326879a0bd2c.jpg',\n",
       " 'friends-happy-190821-1490x838.jpg',\n",
       " 'iblp-members-montage-shiny-happy-people.jpg',\n",
       " 'image28.jpeg',\n",
       " 'business-people-succesful-celebrating-group-successful-39416686-800x500.jpg',\n",
       " 'maxresdefault.jpg',\n",
       " 'rating-victory-isolated-white-background-concept-friendship-healthy-91211177.jpg',\n",
       " '1000_F_42220133_toAU6USGY9jVr2XJFLssfG00cSYIJ173.jpg',\n",
       " 'MV5BZjYwNTc1M2MtM2YxMi00YzYyLTkxY2ItY2I3OWE3NDBmNDVmXkEyXkFqcGdeQXVyMjkwOTAyMDU._V1_.jpg',\n",
       " '20150413185238-secrets-happy-entrepreneurs-woman-gratitude-rainbow-.jpeg',\n",
       " 'happy-person.jpeg',\n",
       " 'image32.jpeg',\n",
       " '360_F_484873483_hg1ofIdXbMha5lKEDG3hJBrwKh1oikTq.jpg',\n",
       " '477876381.jpg',\n",
       " 'image_jumpstory-download20230421-155938_7a7b427.jpg',\n",
       " 'GettyImages-871518740.jpg',\n",
       " '360_F_237330799_fynB7PDF8qXuq2qYTEKOa6E0Lt8Vbzv6.jpg',\n",
       " 'nm-how-happiness-affects-health-tnail.jpg',\n",
       " 'Travis-Bradberry-Happy.jpg',\n",
       " '1HEoLBLidT2u4mhJ0oiDgig.png',\n",
       " 'Screen-Shot-2019-09-10-at-2.39.32-PM-1024x621.png',\n",
       " '360_F_165246984_Ihe5LVattiq8zEPivcPqrtM85x7noWJw.jpg',\n",
       " 'FreeVector-Happy-People-Silhouettes.jpg',\n",
       " 'businesswoman-giving-high-five-male-260nw-2226244055.jpg',\n",
       " 'sddefault.jpg',\n",
       " 'happy-woman-in-nature-at-sunset.jpg',\n",
       " 'image25.jpeg',\n",
       " '_happy_jumping_on_beach-40815.jpg',\n",
       " 'image33.jpeg',\n",
       " 'very-happy-people.jpg',\n",
       " '56f455011e0000b300705475.jpeg',\n",
       " 'happy_1_1678616873966_1678616915228_1678616915228.jpg',\n",
       " 'image29.jpeg',\n",
       " 'A_Sep20_14_1189155141.jpg',\n",
       " '360_F_313450534_bHkt5SoetREYpgWO5uOpceVnaDCngOX7.jpg',\n",
       " 'traitshappypeople.jpg',\n",
       " 'getty_143919450_9706479704500104_51510.jpg',\n",
       " 'image10.jpeg',\n",
       " 'Happy-people-800x533.jpg',\n",
       " 'Screen-Shot-2012-10-23-at-12.57.22-PM.png',\n",
       " 'happy-people21.jpg',\n",
       " 'jumping_for_joy_for_3_kinds_of_happy_people.jpg',\n",
       " 'happy-people-group-fb.jpg',\n",
       " '1000_F_257759165_m9yldegJvu8gPi8lOHDM0mwP8LejhmKK.jpg',\n",
       " 'Happy-People-2.jpeg',\n",
       " 'happy-people.png',\n",
       " 'image30.jpeg',\n",
       " 'Successful-year.jpg',\n",
       " 'image31.jpeg',\n",
       " '7VR73K6EP5ETVEOUFANWBUYJEQ.jpg',\n",
       " 'Happy_guy.jpg',\n",
       " 'image27.jpeg',\n",
       " 'physed-happiness-superJumbo.jpg',\n",
       " '7-Habits-of-Happy-People-image.jpeg',\n",
       " 'Happiness-Habits-10-Things-Happy-People-Do-Before-Bed.jpg',\n",
       " '35438_hd.jpg',\n",
       " 'Woman-Smiling-At-Dinner-Party.jpg',\n",
       " '960x0.jpg',\n",
       " 'jumping-and-dancing-happy-people-positive-emotions-set-illustration-free-vector.jpg',\n",
       " 'dv2051009.jpg',\n",
       " 'image9.jpeg',\n",
       " 'smile.woman_.jpg',\n",
       " '1000_F_38064890_H0vhLXPugCMaxRnEzXtASpmml1NKhqrp.jpg',\n",
       " 'flexible.jpg',\n",
       " 'Happy-Guy.jpg',\n",
       " 'getty_152414899_97046097045006_68075.jpg',\n",
       " 'GettyImages-565706549-949x534.jpg',\n",
       " '1-2.jpg',\n",
       " 'what-makes-people-happy1.jpg',\n",
       " 'Happy20People.jpg',\n",
       " 'getty_505175324_2000131020009280246_158016.jpg',\n",
       " 'happy-people-2.jpg',\n",
       " '4239f3d51d092f69d529f372a37b4601.jpg',\n",
       " 'iStock-1366824713.jpg',\n",
       " 'getty_478389113_970647970450091_99776.jpg',\n",
       " 'smiling-woman_W6GFOSFAXA.jpg',\n",
       " 'Copy-of-Rustic-Female-Teen-Magazine-Cover.jpg',\n",
       " 'Couple_Happy_money_062618.jpg',\n",
       " 'web3-happy-people-outside-smile-sun-nature-eduardo-dutra-620857-unsplash.jpg',\n",
       " 'hand-drawn-happy-people-jumping_23-2149092878.jpg',\n",
       " 'Super-Happy-People-yay.jpg',\n",
       " '71hBPTAhIXL._AC_UF10001000_QL80_.jpg',\n",
       " 'brady-bunch-3.jpg',\n",
       " 'happypeople-1024x679.jpg',\n",
       " 'getty_511990984_130175.jpg',\n",
       " 'guilherme-stecanella-375176-unsplash.jpg',\n",
       " 'image3.jpeg',\n",
       " 'depositphotos_208447156-stock-photo-young-happy-people-have-fun.jpg']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(data_dir, \"Happy people\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_exts = ['jpg', 'jpeg', 'png', 'bmp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid image type: data/Sad people/sad-young-woman-feeling-stressed-footage-107160392_iconl.jpeg\n",
      "Invalid image type: data/Sad people/very-sad-man-sitting-alone-on-white-background-depressed-young-man-sitting-businessman-vector.jpg\n",
      "Invalid image type: data/Sad people/sad-people-are-standing-rooftop-092116870_prevstill.jpeg\n",
      "Invalid image type: data/Happy people/.DS_Store\n",
      "Invalid image type: data/Happy people/7-Habits-of-Happy-People.png\n",
      "Invalid image type: data/Happy people/traitshappypeople.jpg\n",
      "Invalid image type: data/Happy people/Happy20People.jpg\n",
      "Invalid image type: data/Happy people/depositphotos_208447156-stock-photo-young-happy-people-have-fun.jpg\n"
     ]
    }
   ],
   "source": [
    "for imageClass in os.listdir(data_dir):\n",
    "    class_dir = os.path.join(data_dir, imageClass)\n",
    "    if os.path.isdir(class_dir):\n",
    "        for image in os.listdir(class_dir):\n",
    "            image_path = os.path.join(class_dir, image)\n",
    "            try:\n",
    "                img = cv.imread(image_path)\n",
    "                tip = ihdr.what(image_path)\n",
    "                if tip not in img_exts:\n",
    "                    print(f\"Invalid image type: {image_path}\")\n",
    "                    os.remove(image_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Invalid image: {image_path}\")\n",
    "                # os.remove(image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m        \n",
      "\u001b[0;34m@\u001b[0m\u001b[0mtf_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0mDatasetV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcollections_abc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtracking_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrackable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcomposite_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompositeTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdata_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmetaclass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mABCMeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m\"\"\"Represents a potentially large set of elements.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  The `tf.data.Dataset` API supports writing descriptive and efficient input\u001b[0m\n",
      "\u001b[0;34m  pipelines. `Dataset` usage follows a common pattern:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  1. Create a source dataset from your input data.\u001b[0m\n",
      "\u001b[0;34m  2. Apply dataset transformations to preprocess the data.\u001b[0m\n",
      "\u001b[0;34m  3. Iterate over the dataset and process the elements.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  Iteration happens in a streaming fashion, so the full dataset does not need to\u001b[0m\n",
      "\u001b[0;34m  fit into memory.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  Source Datasets:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  The simplest way to create a dataset is to create it from a python `list`:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\u001b[0m\n",
      "\u001b[0;34m  >>> for element in dataset:\u001b[0m\n",
      "\u001b[0;34m  ...   print(element)\u001b[0m\n",
      "\u001b[0;34m  tf.Tensor(1, shape=(), dtype=int32)\u001b[0m\n",
      "\u001b[0;34m  tf.Tensor(2, shape=(), dtype=int32)\u001b[0m\n",
      "\u001b[0;34m  tf.Tensor(3, shape=(), dtype=int32)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  To process lines from files, use `tf.data.TextLineDataset`:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  >>> dataset = tf.data.TextLineDataset([\"file1.txt\", \"file2.txt\"])\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  To process records written in the `TFRecord` format, use `TFRecordDataset`:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  >>> dataset = tf.data.TFRecordDataset([\"file1.tfrecords\", \"file2.tfrecords\"])\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  To create a dataset of all files matching a pattern, use\u001b[0m\n",
      "\u001b[0;34m  `tf.data.Dataset.list_files`:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  ```python\u001b[0m\n",
      "\u001b[0;34m  dataset = tf.data.Dataset.list_files(\"/path/*.txt\")\u001b[0m\n",
      "\u001b[0;34m  ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  See `tf.data.FixedLengthRecordDataset` and `tf.data.Dataset.from_generator`\u001b[0m\n",
      "\u001b[0;34m  for more ways to create datasets.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  Transformations:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  Once you have a dataset, you can apply transformations to prepare the data for\u001b[0m\n",
      "\u001b[0;34m  your model:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\u001b[0m\n",
      "\u001b[0;34m  >>> dataset = dataset.map(lambda x: x*2)\u001b[0m\n",
      "\u001b[0;34m  >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m  [2, 4, 6]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  Common Terms:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  **Element**: A single output from calling `next()` on a dataset iterator.\u001b[0m\n",
      "\u001b[0;34m    Elements may be nested structures containing multiple components. For\u001b[0m\n",
      "\u001b[0;34m    example, the element `(1, (3, \"apple\"))` has one tuple nested in another\u001b[0m\n",
      "\u001b[0;34m    tuple. The components are `1`, `3`, and `\"apple\"`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  **Component**: The leaf in the nested structure of an element.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  Supported types:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  Elements can be nested structures of tuples, named tuples, and dictionaries.\u001b[0m\n",
      "\u001b[0;34m  Note that Python lists are *not* treated as nested structures of components.\u001b[0m\n",
      "\u001b[0;34m  Instead, lists are converted to tensors and treated as components. For\u001b[0m\n",
      "\u001b[0;34m  example, the element `(1, [1, 2, 3])` has only two components; the tensor `1`\u001b[0m\n",
      "\u001b[0;34m  and the tensor `[1, 2, 3]`. Element components can be of any type\u001b[0m\n",
      "\u001b[0;34m  representable by `tf.TypeSpec`, including `tf.Tensor`, `tf.data.Dataset`,\u001b[0m\n",
      "\u001b[0;34m  `tf.sparse.SparseTensor`, `tf.RaggedTensor`, and `tf.TensorArray`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  ```python\u001b[0m\n",
      "\u001b[0;34m  a = 1 # Integer element\u001b[0m\n",
      "\u001b[0;34m  b = 2.0 # Float element\u001b[0m\n",
      "\u001b[0;34m  c = (1, 2) # Tuple element with 2 components\u001b[0m\n",
      "\u001b[0;34m  d = {\"a\": (2, 2), \"b\": 3} # Dict element with 3 components\u001b[0m\n",
      "\u001b[0;34m  Point = collections.namedtuple(\"Point\", [\"x\", \"y\"])\u001b[0m\n",
      "\u001b[0;34m  e = Point(1, 2) # Named tuple\u001b[0m\n",
      "\u001b[0;34m  f = tf.data.Dataset.range(10) # Dataset element\u001b[0m\n",
      "\u001b[0;34m  ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m  For more information,\u001b[0m\n",
      "\u001b[0;34m  read [this guide](https://www.tensorflow.org/guide/data).\u001b[0m\n",
      "\u001b[0;34m  \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Creates a DatasetV2 object.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This is a difference between DatasetV1 and DatasetV2. DatasetV1 does not\u001b[0m\n",
      "\u001b[0;34m    take anything in its constructor whereas in the DatasetV2, we expect\u001b[0m\n",
      "\u001b[0;34m    subclasses to create a variant_tensor and pass it in to the super() call.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      variant_tensor: A DT_VARIANT tensor that represents the dataset.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Initialize the options for this dataset and its inputs.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0minput_dataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0minput_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# If the V1 dataset does not have the `_dataset` attribute, we assume it\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# is a dataset source and hence does not have options. Otherwise, we\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# grab the options of `_dataset` object\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34mf\"Each input of dataset {type(self)} should be a subclass of \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34mf\"`tf.data.Dataset` but encountered \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34mf\"{type(input_dataset._dataset)}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0minput_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options_attr\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0minput_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options_attr\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34mf\"Each input of dataset {type(self)} should be a subclass of \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34mf\"`tf.data.Dataset` but encountered {type(input_dataset)}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mif\u001b[0m \u001b[0minput_options\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options_attr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options_attr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mutable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor_attr\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m_variant_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The `_variant_tensor` property cannot be modified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mdeprecation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecated_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Use external_state_policy instead\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                               \u001b[0;34m\"allow_stateful\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m_as_serialized_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mallow_stateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mstrip_device_assignment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mexternal_state_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExternalStatePolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWARN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Produces serialized graph representation of the dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      allow_stateful: If true, we allow stateful ops to be present in the graph\u001b[0m\n",
      "\u001b[0;34m        def. In that case, the state in these ops would be thrown away.\u001b[0m\n",
      "\u001b[0;34m      strip_device_assignment: If true, non-local (i.e. job and task) device\u001b[0m\n",
      "\u001b[0;34m        assignment is stripped from ops in the serialized graph.\u001b[0m\n",
      "\u001b[0;34m      external_state_policy: The ExternalStatePolicy enum that determines how we\u001b[0m\n",
      "\u001b[0;34m        handle input pipelines that depend on external state. By default, its\u001b[0m\n",
      "\u001b[0;34m        set to WARN.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A scalar `tf.Tensor` of `tf.string` type, representing this dataset as a\u001b[0m\n",
      "\u001b[0;34m      serialized graph.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mexternal_state_policy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexternal_state_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_to_graph_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mexternal_state_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mstrip_device_assignment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrip_device_assignment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mstrip_device_assignment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mallow_stateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_stateful\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mstrip_device_assignment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrip_device_assignment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_stateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_stateful\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m_maybe_track_assets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Finds and tracks nodes in `graph_def` that refer to asset files.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      graph_def: Serialized graph representation of this dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A dictionary mapping the node name of an asset constant to a tracked\u001b[0m\n",
      "\u001b[0;34m      `asset.Asset` object.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0masset_tracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FileIdentity\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0masset_tracker\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0masset_tracker\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0masset_tracker\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtensor_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CPU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mnode_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_parsing_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m              \u001b[0mtensor_proto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0masset_tracker\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_track_trackable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAsset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0masset_tracker\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m_trackable_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                          \u001b[0msave_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtracking_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaveType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHECKPOINT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                          \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0msave_type\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtracking_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaveType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVEDMODEL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# _trace_variant_creation only works when executing eagerly, so we don't\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# want to run it in the object initialization.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mdef_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mresource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trace_variant_creation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0mresource\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0m_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Trigger asset tracking\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mchildren\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trackable_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_variant_tracker\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_VariantTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                                   \u001b[0m_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mchildren\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m_trace_variant_creation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Traces a function which outputs a variant `tf.Tensor` for this dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Note that creating this function involves evaluating an op, and is currently\u001b[0m\n",
      "\u001b[0;34m    only supported when executing eagerly.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A zero-argument `ConcreteFunction` which outputs a variant `tf.Tensor`.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvariant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;34m\"Constructing a tf.function that reproduces a given dataset is only \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;34m\"supported for datasets created eagerly. Please file a feature \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;34m\"request if this is important to you.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CPU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mgraph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_serialized_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexternal_state_policy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions_lib\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                    \u001b[0;34m.\u001b[0m\u001b[0mExternalStatePolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAIL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_node_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"_Retval\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moutput_node_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_node_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;34mf\"Dataset graph is expected to only have one return value but found \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;34mf\"{len(output_node_names)} return values: {output_node_names}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_node_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_node_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfile_path_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# When building a tf.function, track files as `saved_model.Asset`s.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilding_function\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0masset_tracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_track_assets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0masset_tracker\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0massets_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0masset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0masset_tracker\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mfile_path_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massets_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Add functions used in this Dataset to the function's graph, since they\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# need to follow it around (and for example be added to a SavedModel which\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# references the dataset).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvariant_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_from_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_node_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\":0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcaptures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_path_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mused_function\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mused_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariant_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mvariant_function\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Returns a list of the input datasets of the dataset.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{type(self)}._inputs()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_attr\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The `_graph` property cannot be modified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;31m# TODO(jsimsa): Change this to be the transitive closure of functions used\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;31m# by this dataset and its inputs.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mStructuredFunctionWrapper\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Returns a list of functions associated with this dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A list of `StructuredFunctionWrapper` objects.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Returns the options tensor for this dataset.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m_options_tensor_to_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialized_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Converts options tensor to tf.data.Options object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_options\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mpb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_options_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mserialized_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpb\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Returns the options for this dataset and its inputs.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A `tf.data.Options` object representing the dataset options.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options_tensor_to_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mutable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"To make it possible to preserve tf.data options across \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                  \u001b[0;34m\"serialization boundaries, their implementation has moved to \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                  \u001b[0;34m\"be part of the TensorFlow graph. As a consequence, the \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                  \u001b[0;34m\"options value is in general no longer known at graph \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                  \u001b[0;34m\"construction time. Invoking this method in graph mode \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                  \u001b[0;34m\"retains the legacy behavior of the original implementation, \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                  \u001b[0;34m\"but note that the returned value might not reflect the \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                  \u001b[0;34m\"actual value of the options.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options_attr\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m_apply_debug_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mdebug_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEBUG_MODE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;31m# Disable autotuning and static optimizations that could introduce\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;31m# parallelism or asynchrony.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautotune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_optimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_parallelization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_optimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_and_batch_fusion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_optimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_parallelization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_OptionsDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Creates an iterator for elements of this dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The returned iterator implements the Python Iterator protocol.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      An `tf.data.Iterator` for the elements of this dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Raises:\u001b[0m\n",
      "\u001b[0;34m      RuntimeError: If not inside of tf.function and not executing eagerly.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`tf.data.Dataset` only supports Python-style \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                         \u001b[0;34m\"iteration in eager mode or within tf.function.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# Required as __len__ is defined\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0m__nonzero__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__bool__\u001b[0m  \u001b[0;31m# Python 2 backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Returns the length of the dataset if it is known and finite.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This method requires that you are running in eager mode, and that the\u001b[0m\n",
      "\u001b[0;34m    length of the dataset is known and non-infinite. When the length may be\u001b[0m\n",
      "\u001b[0;34m    unknown or infinite, or if you are running in graph mode, use\u001b[0m\n",
      "\u001b[0;34m    `tf.data.Dataset.cardinality` instead.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      An integer representing the length of the dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Raises:\u001b[0m\n",
      "\u001b[0;34m      RuntimeError: If the dataset length is unknown or infinite, or if eager\u001b[0m\n",
      "\u001b[0;34m        execution is not enabled.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`tf.data.Dataset` only supports `len` in eager mode. \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                      \u001b[0;34m\"Use `tf.data.Dataset.cardinality()` instead.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mINFINITE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The dataset is infinite.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mUNKNOWN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The dataset length is unknown.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0melement_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"The type specification of an element of this dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\u001b[0m\n",
      "\u001b[0;34m    >>> dataset.element_spec\u001b[0m\n",
      "\u001b[0;34m    TensorSpec(shape=(), dtype=tf.int32, name=None)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    For more information,\u001b[0m\n",
      "\u001b[0;34m    read [this guide](https://www.tensorflow.org/guide/data#dataset_structure).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A (nested) structure of `tf.TypeSpec` objects matching the structure of an\u001b[0m\n",
      "\u001b[0;34m      element of this dataset and specifying the type of individual components.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{type(self)}.element_spec()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtype_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetV1Adapter\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;34mf\"<{type_.__name__} element_spec={self.element_spec}>\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m__debug_string__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Returns a string showing the type of the dataset and its inputs.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This string is intended only for debugging purposes, and may change without\u001b[0m\n",
      "\u001b[0;34m    warning.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mto_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Stack of (dataset, depth) pairs.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mwhile\u001b[0m \u001b[0mto_process\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mto_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mas_numpy_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Returns an iterator which converts all elements of the dataset to numpy.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Use `as_numpy_iterator` to inspect the content of your dataset. To see\u001b[0m\n",
      "\u001b[0;34m    element shapes and types, print dataset elements directly instead of using\u001b[0m\n",
      "\u001b[0;34m    `as_numpy_iterator`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\u001b[0m\n",
      "\u001b[0;34m    >>> for element in dataset:\u001b[0m\n",
      "\u001b[0;34m    ...   print(element)\u001b[0m\n",
      "\u001b[0;34m    tf.Tensor(1, shape=(), dtype=int32)\u001b[0m\n",
      "\u001b[0;34m    tf.Tensor(2, shape=(), dtype=int32)\u001b[0m\n",
      "\u001b[0;34m    tf.Tensor(3, shape=(), dtype=int32)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This method requires that you are running in eager mode and the dataset's\u001b[0m\n",
      "\u001b[0;34m    element_spec contains only `TensorSpec` components.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\u001b[0m\n",
      "\u001b[0;34m    >>> for element in dataset.as_numpy_iterator():\u001b[0m\n",
      "\u001b[0;34m    ...   print(element)\u001b[0m\n",
      "\u001b[0;34m    1\u001b[0m\n",
      "\u001b[0;34m    2\u001b[0m\n",
      "\u001b[0;34m    3\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\u001b[0m\n",
      "\u001b[0;34m    >>> print(list(dataset.as_numpy_iterator()))\u001b[0m\n",
      "\u001b[0;34m    [1, 2, 3]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    `as_numpy_iterator()` will preserve the nested structure of dataset\u001b[0m\n",
      "\u001b[0;34m    elements.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),\u001b[0m\n",
      "\u001b[0;34m    ...                                               'b': [5, 6]})\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},\u001b[0m\n",
      "\u001b[0;34m    ...                                       {'a': (2, 4), 'b': 6}]\u001b[0m\n",
      "\u001b[0;34m    True\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      An iterable over the elements of the dataset, with their tensors converted\u001b[0m\n",
      "\u001b[0;34m      to numpy arrays.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Raises:\u001b[0m\n",
      "\u001b[0;34m      TypeError: if an element contains a non-`Tensor` value.\u001b[0m\n",
      "\u001b[0;34m      RuntimeError: if eager execution is not enabled.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`tf.data.Dataset.as_numpy_iterator()` is only \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                         \u001b[0;34m\"supported in eager mode.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mcomponent_spec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mcomponent_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;34m(\u001b[0m\u001b[0mtensor_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mragged_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRaggedTensorSpec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m           \u001b[0msparse_tensor_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensorSpec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnone_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoneTensorSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34mf\"`tf.data.Dataset.as_numpy_iterator()` is not supported for \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34mf\"datasets that produce values of type {component_spec.value_type}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mNumpyIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m_flat_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Returns a list `tf.TensorShapes`s for the element tensor representation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A list `tf.TensorShapes`s for the element tensor representation.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flat_tensor_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m_flat_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Returns a list `tf.DType`s for the element tensor representation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A list `tf.DType`s for the element tensor representation.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flat_tensor_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m_flat_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Helper for setting `output_shapes` and `output_types` attrs of an op.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Most dataset op constructors expect `output_shapes` and `output_types`\u001b[0m\n",
      "\u001b[0;34m    arguments that represent the flattened structure of an element. This helper\u001b[0m\n",
      "\u001b[0;34m    function generates these attrs as a keyword argument dictionary, allowing\u001b[0m\n",
      "\u001b[0;34m    `Dataset._variant_tensor` implementations to pass `**self._flat_structure`\u001b[0m\n",
      "\u001b[0;34m    to the op constructor.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A dictionary of keyword arguments that can be passed to a dataset op\u001b[0m\n",
      "\u001b[0;34m      constructor.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Helper for generating dataset metadata.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_metadata_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_and_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m_common_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Helper for generating arguments that are common across most dataset ops.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Most dataset op constructors expect `output_shapes` and `output_types`\u001b[0m\n",
      "\u001b[0;34m    arguments that represent the flattened structure of an element, as well as a\u001b[0m\n",
      "\u001b[0;34m    `metadata` argument for additional metadata such as user-defined dataset\u001b[0m\n",
      "\u001b[0;34m    name. This helper function generates common attributes as a keyword argument\u001b[0m\n",
      "\u001b[0;34m    dictionary, allowing `Dataset._variant_tensor` implementations to pass\u001b[0m\n",
      "\u001b[0;34m    `**self._common_args` to the op constructor.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A dictionary of keyword arguments that can be passed to a dataset op\u001b[0m\n",
      "\u001b[0;34m      constructor.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0m_type_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Creates a `Dataset` with a single element, comprising the given tensors.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    `from_tensors` produces a dataset containing only a single element. To slice\u001b[0m\n",
      "\u001b[0;34m    the input tensor into multiple elements, use `from_tensor_slices` instead.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensors([1, 2, 3])\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [array([1, 2, 3], dtype=int32)]\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensors(([1, 2, 3], 'A'))\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [(array([1, 2, 3], dtype=int32), b'A')]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> # You can use `from_tensors` to produce a dataset which repeats\u001b[0m\n",
      "\u001b[0;34m    >>> # the same example many times.\u001b[0m\n",
      "\u001b[0;34m    >>> example = tf.constant([1,2,3])\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensors(example).repeat(2)\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [array([1, 2, 3], dtype=int32), array([1, 2, 3], dtype=int32)]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Note that if `tensors` contains a NumPy array, and eager execution is not\u001b[0m\n",
      "\u001b[0;34m    enabled, the values will be embedded in the graph as one or more\u001b[0m\n",
      "\u001b[0;34m    `tf.constant` operations. For large datasets (> 1 GB), this can waste\u001b[0m\n",
      "\u001b[0;34m    memory and run into byte limits of graph serialization. If `tensors`\u001b[0m\n",
      "\u001b[0;34m    contains one or more large NumPy arrays, consider the alternative described\u001b[0m\n",
      "\u001b[0;34m    in [this\u001b[0m\n",
      "\u001b[0;34m    guide](https://tensorflow.org/guide/data#consuming_numpy_arrays).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      tensors: A dataset \"element\". Supported values are documented\u001b[0m\n",
      "\u001b[0;34m        [here](https://www.tensorflow.org/guide/data#dataset_structure).\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      Dataset: A `Dataset`.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# from_tensors_op -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_tensors_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mfrom_tensors_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Creates a `Dataset` whose elements are slices of the given tensors.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The given tensors are sliced along their first dimension. This operation\u001b[0m\n",
      "\u001b[0;34m    preserves the structure of the input tensors, removing the first dimension\u001b[0m\n",
      "\u001b[0;34m    of each tensor and using it as the dataset dimension. All input tensors\u001b[0m\n",
      "\u001b[0;34m    must have the same size in their first dimensions.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> # Slicing a 1D tensor produces scalar tensor elements.\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [1, 2, 3]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> # Slicing a 2D tensor produces 1D tensor elements.\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensor_slices([[1, 2], [3, 4]])\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [array([1, 2], dtype=int32), array([3, 4], dtype=int32)]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> # Slicing a tuple of 1D tensors produces tuple elements containing\u001b[0m\n",
      "\u001b[0;34m    >>> # scalar tensors.\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensor_slices(([1, 2], [3, 4], [5, 6]))\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [(1, 3, 5), (2, 4, 6)]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> # Dictionary structure is also preserved.\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensor_slices({\"a\": [1, 2], \"b\": [3, 4]})\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator()) == [{'a': 1, 'b': 3},\u001b[0m\n",
      "\u001b[0;34m    ...                                       {'a': 2, 'b': 4}]\u001b[0m\n",
      "\u001b[0;34m    True\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> # Two tensors can be combined into one Dataset object.\u001b[0m\n",
      "\u001b[0;34m    >>> features = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==> 3x2 tensor\u001b[0m\n",
      "\u001b[0;34m    >>> labels = tf.constant(['A', 'B', 'A']) # ==> 3x1 tensor\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = Dataset.from_tensor_slices((features, labels))\u001b[0m\n",
      "\u001b[0;34m    >>> # Both the features and the labels tensors can be converted\u001b[0m\n",
      "\u001b[0;34m    >>> # to a Dataset object separately and combined after.\u001b[0m\n",
      "\u001b[0;34m    >>> features_dataset = Dataset.from_tensor_slices(features)\u001b[0m\n",
      "\u001b[0;34m    >>> labels_dataset = Dataset.from_tensor_slices(labels)\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = Dataset.zip((features_dataset, labels_dataset))\u001b[0m\n",
      "\u001b[0;34m    >>> # A batched feature and label set can be converted to a Dataset\u001b[0m\n",
      "\u001b[0;34m    >>> # in similar fashion.\u001b[0m\n",
      "\u001b[0;34m    >>> batched_features = tf.constant([[[1, 3], [2, 3]],\u001b[0m\n",
      "\u001b[0;34m    ...                                 [[2, 1], [1, 2]],\u001b[0m\n",
      "\u001b[0;34m    ...                                 [[3, 3], [3, 2]]], shape=(3, 2, 2))\u001b[0m\n",
      "\u001b[0;34m    >>> batched_labels = tf.constant([['A', 'A'],\u001b[0m\n",
      "\u001b[0;34m    ...                               ['B', 'B'],\u001b[0m\n",
      "\u001b[0;34m    ...                               ['A', 'B']], shape=(3, 2, 1))\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = Dataset.from_tensor_slices((batched_features, batched_labels))\u001b[0m\n",
      "\u001b[0;34m    >>> for element in dataset.as_numpy_iterator():\u001b[0m\n",
      "\u001b[0;34m    ...   print(element)\u001b[0m\n",
      "\u001b[0;34m    (array([[1, 3],\u001b[0m\n",
      "\u001b[0;34m           [2, 3]], dtype=int32), array([[b'A'],\u001b[0m\n",
      "\u001b[0;34m           [b'A']], dtype=object))\u001b[0m\n",
      "\u001b[0;34m    (array([[2, 1],\u001b[0m\n",
      "\u001b[0;34m           [1, 2]], dtype=int32), array([[b'B'],\u001b[0m\n",
      "\u001b[0;34m           [b'B']], dtype=object))\u001b[0m\n",
      "\u001b[0;34m    (array([[3, 3],\u001b[0m\n",
      "\u001b[0;34m           [3, 2]], dtype=int32), array([[b'A'],\u001b[0m\n",
      "\u001b[0;34m           [b'B']], dtype=object))\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Note that if `tensors` contains a NumPy array, and eager execution is not\u001b[0m\n",
      "\u001b[0;34m    enabled, the values will be embedded in the graph as one or more\u001b[0m\n",
      "\u001b[0;34m    `tf.constant` operations. For large datasets (> 1 GB), this can waste\u001b[0m\n",
      "\u001b[0;34m    memory and run into byte limits of graph serialization. If `tensors`\u001b[0m\n",
      "\u001b[0;34m    contains one or more large NumPy arrays, consider the alternative described\u001b[0m\n",
      "\u001b[0;34m    in [this guide](\u001b[0m\n",
      "\u001b[0;34m    https://tensorflow.org/guide/data#consuming_numpy_arrays).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      tensors: A dataset element, whose components have the same first\u001b[0m\n",
      "\u001b[0;34m        dimension. Supported values are documented\u001b[0m\n",
      "\u001b[0;34m        [here](https://www.tensorflow.org/guide/data#dataset_structure).\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      Dataset: A `Dataset`.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# from_tensor_slices_op -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Stores outstanding iterators created from a Python generator.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This class keeps track of potentially multiple iterators that may have\u001b[0m\n",
      "\u001b[0;34m    been created from a generator, e.g. in the case that the dataset is\u001b[0m\n",
      "\u001b[0;34m    repeated, or nested within a parallel computation.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m  \u001b[0;31m# GUARDED_BY(self._lock)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_normalize_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;31m# In debug mode, iterator ids may be eagerly-generated np.arrays instead\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;31m# of Tensors. We convert them to scalars to make them hashable.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0miterator_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0miterator_id\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mget_next_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_id\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_id\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;31m# NOTE(mrry): Explicitly create an array of `np.int64` because implicit\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;31m# casting in `py_func()` will create an array of `np.int32` on Windows,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;31m# leading to a runtime error.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mget_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0miterator_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalize_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miterator_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miterator_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0miterator_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalize_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mdeprecation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecated_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Use output_signature instead\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                               \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mfrom_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0moutput_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0moutput_signature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Creates a `Dataset` whose elements are generated by `generator`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Note: The current implementation of `Dataset.from_generator()` uses\u001b[0m\n",
      "\u001b[0;34m    `tf.numpy_function` and inherits the same constraints. In particular, it\u001b[0m\n",
      "\u001b[0;34m    requires the dataset and iterator related operations to be placed\u001b[0m\n",
      "\u001b[0;34m    on a device in the same process as the Python program that called\u001b[0m\n",
      "\u001b[0;34m    `Dataset.from_generator()`. In particular, using `from_generator` will\u001b[0m\n",
      "\u001b[0;34m    preclude the use of tf.data service for scaling out dataset processing.\u001b[0m\n",
      "\u001b[0;34m    The body of `generator` will not be serialized in a `GraphDef`, and you\u001b[0m\n",
      "\u001b[0;34m    should not use this method if you need to serialize your model and restore\u001b[0m\n",
      "\u001b[0;34m    it in a different environment.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The `generator` argument must be a callable object that returns\u001b[0m\n",
      "\u001b[0;34m    an object that supports the `iter()` protocol (e.g. a generator function).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The elements generated by `generator` must be compatible with either the\u001b[0m\n",
      "\u001b[0;34m    given `output_signature` argument or with the given `output_types` and\u001b[0m\n",
      "\u001b[0;34m    (optionally) `output_shapes` arguments, whichever was specified.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The recommended way to call `from_generator` is to use the\u001b[0m\n",
      "\u001b[0;34m    `output_signature` argument. In this case the output will be assumed to\u001b[0m\n",
      "\u001b[0;34m    consist of objects with the classes, shapes and types defined by\u001b[0m\n",
      "\u001b[0;34m    `tf.TypeSpec` objects from `output_signature` argument:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> def gen():\u001b[0m\n",
      "\u001b[0;34m    ...   ragged_tensor = tf.ragged.constant([[1, 2], [3]])\u001b[0m\n",
      "\u001b[0;34m    ...   yield 42, ragged_tensor\u001b[0m\n",
      "\u001b[0;34m    >>>\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_generator(\u001b[0m\n",
      "\u001b[0;34m    ...      gen,\u001b[0m\n",
      "\u001b[0;34m    ...      output_signature=(\u001b[0m\n",
      "\u001b[0;34m    ...          tf.TensorSpec(shape=(), dtype=tf.int32),\u001b[0m\n",
      "\u001b[0;34m    ...          tf.RaggedTensorSpec(shape=(2, None), dtype=tf.int32)))\u001b[0m\n",
      "\u001b[0;34m    >>>\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.take(1))\u001b[0m\n",
      "\u001b[0;34m    [(<tf.Tensor: shape=(), dtype=int32, numpy=42>,\u001b[0m\n",
      "\u001b[0;34m    <tf.RaggedTensor [[1, 2], [3]]>)]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    There is also a deprecated way to call `from_generator` by either with\u001b[0m\n",
      "\u001b[0;34m    `output_types` argument alone or together with `output_shapes` argument.\u001b[0m\n",
      "\u001b[0;34m    In this case the output of the function will be assumed to consist of\u001b[0m\n",
      "\u001b[0;34m    `tf.Tensor` objects with the types defined by `output_types` and with the\u001b[0m\n",
      "\u001b[0;34m    shapes which are either unknown or defined by `output_shapes`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Note: If `generator` depends on mutable global variables or other external\u001b[0m\n",
      "\u001b[0;34m    state, be aware that the runtime may invoke `generator` multiple times\u001b[0m\n",
      "\u001b[0;34m    (in order to support repeating the `Dataset`) and at any time\u001b[0m\n",
      "\u001b[0;34m    between the call to `Dataset.from_generator()` and the production of the\u001b[0m\n",
      "\u001b[0;34m    first element from the generator. Mutating global variables or external\u001b[0m\n",
      "\u001b[0;34m    state can cause undefined behavior, and we recommend that you explicitly\u001b[0m\n",
      "\u001b[0;34m    cache any external state in `generator` before calling\u001b[0m\n",
      "\u001b[0;34m    `Dataset.from_generator()`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Note: While the `output_signature` parameter makes it possible to yield\u001b[0m\n",
      "\u001b[0;34m    `Dataset` elements, the scope of `Dataset.from_generator()` should be\u001b[0m\n",
      "\u001b[0;34m    limited to logic that cannot be expressed through tf.data operations. Using\u001b[0m\n",
      "\u001b[0;34m    tf.data operations within the generator function is an anti-pattern and may\u001b[0m\n",
      "\u001b[0;34m    result in incremental memory growth.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      generator: A callable object that returns an object that supports the\u001b[0m\n",
      "\u001b[0;34m        `iter()` protocol. If `args` is not specified, `generator` must take no\u001b[0m\n",
      "\u001b[0;34m        arguments; otherwise it must take as many arguments as there are values\u001b[0m\n",
      "\u001b[0;34m        in `args`.\u001b[0m\n",
      "\u001b[0;34m      output_types: (Optional.) A (nested) structure of `tf.DType` objects\u001b[0m\n",
      "\u001b[0;34m        corresponding to each component of an element yielded by `generator`.\u001b[0m\n",
      "\u001b[0;34m      output_shapes: (Optional.) A (nested) structure of `tf.TensorShape`\u001b[0m\n",
      "\u001b[0;34m        objects corresponding to each component of an element yielded by\u001b[0m\n",
      "\u001b[0;34m        `generator`.\u001b[0m\n",
      "\u001b[0;34m      args: (Optional.) A tuple of `tf.Tensor` objects that will be evaluated\u001b[0m\n",
      "\u001b[0;34m        and passed to `generator` as NumPy-array arguments.\u001b[0m\n",
      "\u001b[0;34m      output_signature: (Optional.) A (nested) structure of `tf.TypeSpec`\u001b[0m\n",
      "\u001b[0;34m        objects corresponding to each component of an element yielded by\u001b[0m\n",
      "\u001b[0;34m        `generator`.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operations used by\u001b[0m\n",
      "\u001b[0;34m        `from_generator`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      Dataset: A `Dataset`.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# from_generator_op -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_generator_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mfrom_generator_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                             \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                             \u001b[0moutput_signature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Creates a `Dataset` of a step-separated range of values.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> list(Dataset.range(5).as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [0, 1, 2, 3, 4]\u001b[0m\n",
      "\u001b[0;34m    >>> list(Dataset.range(2, 5).as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [2, 3, 4]\u001b[0m\n",
      "\u001b[0;34m    >>> list(Dataset.range(1, 5, 2).as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [1, 3]\u001b[0m\n",
      "\u001b[0;34m    >>> list(Dataset.range(1, 5, -2).as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    []\u001b[0m\n",
      "\u001b[0;34m    >>> list(Dataset.range(5, 1).as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    []\u001b[0m\n",
      "\u001b[0;34m    >>> list(Dataset.range(5, 1, -2).as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [5, 3]\u001b[0m\n",
      "\u001b[0;34m    >>> list(Dataset.range(2, 5, output_type=tf.int32).as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [2, 3, 4]\u001b[0m\n",
      "\u001b[0;34m    >>> list(Dataset.range(1, 5, 2, output_type=tf.float32).as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [1.0, 3.0]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      *args: follows the same semantics as python's range.\u001b[0m\n",
      "\u001b[0;34m        len(args) == 1 -> start = 0, stop = args[0], step = 1.\u001b[0m\n",
      "\u001b[0;34m        len(args) == 2 -> start = args[0], stop = args[1], step = 1.\u001b[0m\n",
      "\u001b[0;34m        len(args) == 3 -> start = args[0], stop = args[1], step = args[2].\u001b[0m\n",
      "\u001b[0;34m      **kwargs:\u001b[0m\n",
      "\u001b[0;34m        - output_type: Its expected dtype. (Optional, default: `tf.int64`).\u001b[0m\n",
      "\u001b[0;34m        - name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      Dataset: A `RangeDataset`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Raises:\u001b[0m\n",
      "\u001b[0;34m      ValueError: if len(args) == 0.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops -> range_op ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mrange_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Creates a `Dataset` by zipping together the given datasets.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This method has similar semantics to the built-in `zip()` function\u001b[0m\n",
      "\u001b[0;34m    in Python, with the main difference being that the `datasets`\u001b[0m\n",
      "\u001b[0;34m    argument can be a (nested) structure of `Dataset` objects. The supported\u001b[0m\n",
      "\u001b[0;34m    nesting mechanisms are documented\u001b[0m\n",
      "\u001b[0;34m    [here] (https://www.tensorflow.org/guide/data#dataset_structure).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> # The datasets or nested structure of datasets `*args` argument\u001b[0m\n",
      "\u001b[0;34m    >>> # determines the structure of elements in the resulting dataset.\u001b[0m\n",
      "\u001b[0;34m    >>> a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\u001b[0m\n",
      "\u001b[0;34m    >>> b = tf.data.Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]\u001b[0m\n",
      "\u001b[0;34m    >>> ds = tf.data.Dataset.zip(a, b)\u001b[0m\n",
      "\u001b[0;34m    >>> list(ds.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [(1, 4), (2, 5), (3, 6)]\u001b[0m\n",
      "\u001b[0;34m    >>> ds = tf.data.Dataset.zip(b, a)\u001b[0m\n",
      "\u001b[0;34m    >>> list(ds.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [(4, 1), (5, 2), (6, 3)]\u001b[0m\n",
      "\u001b[0;34m    >>>\u001b[0m\n",
      "\u001b[0;34m    >>> # The `datasets` argument may contain an arbitrary number of datasets.\u001b[0m\n",
      "\u001b[0;34m    >>> c = tf.data.Dataset.range(7, 13).batch(2)  # ==> [ [7, 8],\u001b[0m\n",
      "\u001b[0;34m    ...                                            #       [9, 10],\u001b[0m\n",
      "\u001b[0;34m    ...                                            #       [11, 12] ]\u001b[0m\n",
      "\u001b[0;34m    >>> ds = tf.data.Dataset.zip(a, b, c)\u001b[0m\n",
      "\u001b[0;34m    >>> for element in ds.as_numpy_iterator():\u001b[0m\n",
      "\u001b[0;34m    ...   print(element)\u001b[0m\n",
      "\u001b[0;34m    (1, 4, array([7, 8]))\u001b[0m\n",
      "\u001b[0;34m    (2, 5, array([ 9, 10]))\u001b[0m\n",
      "\u001b[0;34m    (3, 6, array([11, 12]))\u001b[0m\n",
      "\u001b[0;34m    >>>\u001b[0m\n",
      "\u001b[0;34m    >>> # The number of elements in the resulting dataset is the same as\u001b[0m\n",
      "\u001b[0;34m    >>> # the size of the smallest dataset in `datasets`.\u001b[0m\n",
      "\u001b[0;34m    >>> d = tf.data.Dataset.range(13, 15)  # ==> [ 13, 14 ]\u001b[0m\n",
      "\u001b[0;34m    >>> ds = tf.data.Dataset.zip(a, d)\u001b[0m\n",
      "\u001b[0;34m    >>> list(ds.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [(1, 13), (2, 14)]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      *args: Datasets or nested structures of datasets to zip together. This\u001b[0m\n",
      "\u001b[0;34m        can't be set if `datasets` is set.\u001b[0m\n",
      "\u001b[0;34m      datasets: A (nested) structure of datasets. This can't be set if `*args`\u001b[0m\n",
      "\u001b[0;34m        is set. Note that this exists only for backwards compatibility and it is\u001b[0m\n",
      "\u001b[0;34m        preferred to use *args.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops -> zip_op ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzip_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Must pass at least one dataset to `zip`.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Both `*args` and `datasets` cannot be set.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mzip_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Creates a `Dataset` by concatenating the given dataset with this dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\u001b[0m\n",
      "\u001b[0;34m    >>> b = tf.data.Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ]\u001b[0m\n",
      "\u001b[0;34m    >>> ds = a.concatenate(b)\u001b[0m\n",
      "\u001b[0;34m    >>> list(ds.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [1, 2, 3, 4, 5, 6, 7]\u001b[0m\n",
      "\u001b[0;34m    >>> # The input dataset and dataset to be concatenated should have\u001b[0m\n",
      "\u001b[0;34m    >>> # compatible element specs.\u001b[0m\n",
      "\u001b[0;34m    >>> c = tf.data.Dataset.zip((a, b))\u001b[0m\n",
      "\u001b[0;34m    >>> a.concatenate(c)\u001b[0m\n",
      "\u001b[0;34m    Traceback (most recent call last):\u001b[0m\n",
      "\u001b[0;34m    TypeError: Two datasets to concatenate have different types\u001b[0m\n",
      "\u001b[0;34m    <dtype: 'int64'> and (tf.int64, tf.int64)\u001b[0m\n",
      "\u001b[0;34m    >>> d = tf.data.Dataset.from_tensor_slices([\"a\", \"b\", \"c\"])\u001b[0m\n",
      "\u001b[0;34m    >>> a.concatenate(d)\u001b[0m\n",
      "\u001b[0;34m    Traceback (most recent call last):\u001b[0m\n",
      "\u001b[0;34m    TypeError: Two datasets to concatenate have different types\u001b[0m\n",
      "\u001b[0;34m    <dtype: 'int64'> and <dtype: 'string'>\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      dataset: `Dataset` to be concatenated.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# concatenate_op -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconcatenate_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Creates a `Dataset` that counts from `start` in steps of size `step`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Unlike `tf.data.Dataset.range`, which stops at some ending number,\u001b[0m\n",
      "\u001b[0;34m    `tf.data.Dataset.counter` produces elements indefinitely.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.experimental.Counter().take(5)\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [0, 1, 2, 3, 4]\u001b[0m\n",
      "\u001b[0;34m    >>> dataset.element_spec\u001b[0m\n",
      "\u001b[0;34m    TensorSpec(shape=(), dtype=tf.int64, name=None)\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.experimental.Counter(dtype=tf.int32)\u001b[0m\n",
      "\u001b[0;34m    >>> dataset.element_spec\u001b[0m\n",
      "\u001b[0;34m    TensorSpec(shape=(), dtype=tf.int32, name=None)\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.experimental.Counter(start=2).take(5)\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [2, 3, 4, 5, 6]\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.experimental.Counter(start=2, step=5).take(5)\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [2, 7, 12, 17, 22]\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.experimental.Counter(start=10, step=-1).take(5)\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [10, 9, 8, 7, 6]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      start: (Optional.) The starting value for the counter. Defaults to 0.\u001b[0m\n",
      "\u001b[0;34m      step: (Optional.) The step size for the counter. Defaults to 1.\u001b[0m\n",
      "\u001b[0;34m      dtype: (Optional.) The data type for counter elements. Defaults to\u001b[0m\n",
      "\u001b[0;34m        `tf.int64`.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A `Dataset` of scalar `dtype` elements.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops -> counter_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcounter_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mcounter_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mfingerprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Computes the fingerprint of this `Dataset`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    If two datasets have the same fingerprint, it is guaranteeed that they\u001b[0m\n",
      "\u001b[0;34m    would produce identical elements as long as the content of the upstream\u001b[0m\n",
      "\u001b[0;34m    input files does not change and they produce data deterministically.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    However, two datasets producing identical values does not always mean they\u001b[0m\n",
      "\u001b[0;34m    would have the same fingerprint due to different graph constructs.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    In other words, if two datasets have different fingerprints, they could\u001b[0m\n",
      "\u001b[0;34m    still produce identical values.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A scalar `tf.Tensor` of type `tf.uint64`.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_fingerprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mrebatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Creates a `Dataset` that rebatches the elements from this dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    `rebatch(N)` is functionally equivalent to `unbatch().batch(N)`, but is\u001b[0m\n",
      "\u001b[0;34m    more efficient, performing one copy instead of two.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> ds = tf.data.Dataset.range(6)\u001b[0m\n",
      "\u001b[0;34m    >>> ds = ds.batch(2)\u001b[0m\n",
      "\u001b[0;34m    >>> ds = ds.rebatch(3)\u001b[0m\n",
      "\u001b[0;34m    >>> list(ds.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [array([0, 1, 2]), array([3, 4, 5])]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> ds = tf.data.Dataset.range(7)\u001b[0m\n",
      "\u001b[0;34m    >>> ds = ds.batch(4)\u001b[0m\n",
      "\u001b[0;34m    >>> ds = ds.rebatch(3)\u001b[0m\n",
      "\u001b[0;34m    >>> list(ds.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [array([0, 1, 2]), array([3, 4, 5]), array([6])]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> ds = tf.data.Dataset.range(7)\u001b[0m\n",
      "\u001b[0;34m    >>> ds = ds.batch(2)\u001b[0m\n",
      "\u001b[0;34m    >>> ds = ds.rebatch(3, drop_remainder=True)\u001b[0m\n",
      "\u001b[0;34m    >>> list(ds.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [array([0, 1, 2]), array([3, 4, 5])]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    If the `batch_size` argument is a list, `rebatch` cycles through the list\u001b[0m\n",
      "\u001b[0;34m    to determine the size of each batch.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> ds = tf.data.Dataset.range(8)\u001b[0m\n",
      "\u001b[0;34m    >>> ds = ds.batch(4)\u001b[0m\n",
      "\u001b[0;34m    >>> ds = ds.rebatch([2, 1, 1])\u001b[0m\n",
      "\u001b[0;34m    >>> list(ds.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [array([0, 1]), array([2]), array([3]), array([4, 5]), array([6]),\u001b[0m\n",
      "\u001b[0;34m    array([7])]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      batch_size: A `tf.int64` scalar or vector, representing the size of\u001b[0m\n",
      "\u001b[0;34m        batches to produce. If this argument is a vector, these values are\u001b[0m\n",
      "\u001b[0;34m        cycled through in round robin fashion.\u001b[0m\n",
      "\u001b[0;34m      drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\u001b[0m\n",
      "\u001b[0;34m        whether the last batch should be dropped in the case it has fewer than\u001b[0m\n",
      "\u001b[0;34m        `batch_size[cycle_index]` elements; the default behavior is not to drop\u001b[0m\n",
      "\u001b[0;34m        the smaller batch.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A `Dataset` of scalar `dtype` elements.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops -> rebatch_op ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# rebatch_op -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrebatch_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mrebatch_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rebatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Creates a `Dataset` that prefetches elements from this dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Most dataset input pipelines should end with a call to `prefetch`. This\u001b[0m\n",
      "\u001b[0;34m    allows later elements to be prepared while the current element is being\u001b[0m\n",
      "\u001b[0;34m    processed. This often improves latency and throughput, at the cost of\u001b[0m\n",
      "\u001b[0;34m    using additional memory to store prefetched elements.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Note: Like other `Dataset` methods, prefetch operates on the\u001b[0m\n",
      "\u001b[0;34m    elements of the input dataset. It has no concept of examples vs. batches.\u001b[0m\n",
      "\u001b[0;34m    `examples.prefetch(2)` will prefetch two elements (2 examples),\u001b[0m\n",
      "\u001b[0;34m    while `examples.batch(20).prefetch(2)` will prefetch 2 elements\u001b[0m\n",
      "\u001b[0;34m    (2 batches, of 20 examples each).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.range(3)\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.prefetch(2)\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [0, 1, 2]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      buffer_size: A `tf.int64` scalar `tf.Tensor`, representing the maximum\u001b[0m\n",
      "\u001b[0;34m        number of elements that will be buffered when prefetching. If the value\u001b[0m\n",
      "\u001b[0;34m        `tf.data.AUTOTUNE` is used, then the buffer size is dynamically tuned.\u001b[0m\n",
      "\u001b[0;34m      name: Optional. A name for the tf.data transformation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mprefetch_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prefetch\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mlist_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mfile_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"A dataset of all files matching one or more glob patterns.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The `file_pattern` argument should be a small number of glob patterns.\u001b[0m\n",
      "\u001b[0;34m    If your filenames have already been globbed, use\u001b[0m\n",
      "\u001b[0;34m    `Dataset.from_tensor_slices(filenames)` instead, as re-globbing every\u001b[0m\n",
      "\u001b[0;34m    filename with `list_files` may result in poor performance with remote\u001b[0m\n",
      "\u001b[0;34m    storage systems.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Note: The default behavior of this method is to return filenames in\u001b[0m\n",
      "\u001b[0;34m    a non-deterministic random shuffled order. Pass a `seed` or `shuffle=False`\u001b[0m\n",
      "\u001b[0;34m    to get results in a deterministic order.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Example:\u001b[0m\n",
      "\u001b[0;34m      If we had the following files on our filesystem:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        - /path/to/dir/a.txt\u001b[0m\n",
      "\u001b[0;34m        - /path/to/dir/b.py\u001b[0m\n",
      "\u001b[0;34m        - /path/to/dir/c.py\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m      If we pass \"/path/to/dir/*.py\" as the directory, the dataset\u001b[0m\n",
      "\u001b[0;34m      would produce:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        - /path/to/dir/b.py\u001b[0m\n",
      "\u001b[0;34m        - /path/to/dir/c.py\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      file_pattern: A string, a list of strings, or a `tf.Tensor` of string type\u001b[0m\n",
      "\u001b[0;34m        (scalar or vector), representing the filename glob (i.e. shell wildcard)\u001b[0m\n",
      "\u001b[0;34m        pattern(s) that will be matched.\u001b[0m\n",
      "\u001b[0;34m      shuffle: (Optional.) If `True`, the file names will be shuffled randomly.\u001b[0m\n",
      "\u001b[0;34m        Defaults to `True`.\u001b[0m\n",
      "\u001b[0;34m      seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the random\u001b[0m\n",
      "\u001b[0;34m        seed that will be used to create the distribution. See\u001b[0m\n",
      "\u001b[0;34m        `tf.random.set_seed` for behavior.\u001b[0m\n",
      "\u001b[0;34m      name: Optional. A name for the tf.data operations used by `list_files`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m     Dataset: A `Dataset` of strings corresponding to file names.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"list_files\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mfile_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mfile_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"file_pattern\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mmatching_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_io_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatching_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;31m# Raise an exception if `file_pattern` does not match any files.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mcondition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgreater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                   \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"match_not_empty\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;34m\"No files matched pattern: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mstring_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_join\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\", \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"message\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0massert_not_empty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrol_flow_assert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAssert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"assert_not_empty\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massert_not_empty\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmatching_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;31m# TODO(b/240947712): Remove lazy import after this method is factored out.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;31m# from_tensor_slices_op -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mmatching_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetV1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetV1Adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# NOTE(mrry): The shuffle buffer size must be greater than zero, but the\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# list of files might be empty.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mbuffer_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Repeats this dataset so each original value is seen `count` times.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.repeat(3)\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [1, 2, 3, 1, 2, 3, 1, 2, 3]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Note: If the input dataset depends on global state (e.g. a random number\u001b[0m\n",
      "\u001b[0;34m    generator) or its output is non-deterministic (e.g. because of upstream\u001b[0m\n",
      "\u001b[0;34m    `shuffle`), then different repetitions may produce different elements.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      count: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\u001b[0m\n",
      "\u001b[0;34m        number of times the dataset should be repeated. The default behavior (if\u001b[0m\n",
      "\u001b[0;34m        `count` is `None` or `-1`) is for the dataset be repeated indefinitely.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops -> repeat_op ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access,redefined-outer-name\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrepeat_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mrepeat_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_repeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access,redefined-outer-name\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Enumerates the elements of this dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    It is similar to python's `enumerate`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.enumerate(start=5)\u001b[0m\n",
      "\u001b[0;34m    >>> for element in dataset.as_numpy_iterator():\u001b[0m\n",
      "\u001b[0;34m    ...   print(element)\u001b[0m\n",
      "\u001b[0;34m    (5, 1)\u001b[0m\n",
      "\u001b[0;34m    (6, 2)\u001b[0m\n",
      "\u001b[0;34m    (7, 3)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> # The (nested) structure of the input dataset determines the\u001b[0m\n",
      "\u001b[0;34m    >>> # structure of elements in the resulting dataset.\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensor_slices([(7, 8), (9, 10)])\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.enumerate()\u001b[0m\n",
      "\u001b[0;34m    >>> for element in dataset.as_numpy_iterator():\u001b[0m\n",
      "\u001b[0;34m    ...   print(element)\u001b[0m\n",
      "\u001b[0;34m    (0, array([7, 8], dtype=int32))\u001b[0m\n",
      "\u001b[0;34m    (1, array([ 9, 10], dtype=int32))\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      start: A `tf.int64` scalar `tf.Tensor`, representing the start value for\u001b[0m\n",
      "\u001b[0;34m        enumeration.\u001b[0m\n",
      "\u001b[0;34m      name: Optional. A name for the tf.data operations used by `enumerate`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrange_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Replicate the range component so that each split is enumerated\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# independently. This avoids the need for prohibitively expensive\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# cross-split coordination.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrange_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_apply_rewrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"replicate_on_split\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshuffle_each_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Randomly shuffles the elements of this dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This dataset fills a buffer with `buffer_size` elements, then randomly\u001b[0m\n",
      "\u001b[0;34m    samples elements from this buffer, replacing the selected elements with new\u001b[0m\n",
      "\u001b[0;34m    elements. For perfect shuffling, a buffer size greater than or equal to the\u001b[0m\n",
      "\u001b[0;34m    full size of the dataset is required.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    For instance, if your dataset contains 10,000 elements but `buffer_size` is\u001b[0m\n",
      "\u001b[0;34m    set to 1,000, then `shuffle` will initially select a random element from\u001b[0m\n",
      "\u001b[0;34m    only the first 1,000 elements in the buffer. Once an element is selected,\u001b[0m\n",
      "\u001b[0;34m    its space in the buffer is replaced by the next (i.e. 1,001-st) element,\u001b[0m\n",
      "\u001b[0;34m    maintaining the 1,000 element buffer.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    `reshuffle_each_iteration` controls whether the shuffle order should be\u001b[0m\n",
      "\u001b[0;34m    different for each epoch. In TF 1.X, the idiomatic way to create epochs\u001b[0m\n",
      "\u001b[0;34m    was through the `repeat` transformation:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```python\u001b[0m\n",
      "\u001b[0;34m    dataset = tf.data.Dataset.range(3)\u001b[0m\n",
      "\u001b[0;34m    dataset = dataset.shuffle(3, reshuffle_each_iteration=True)\u001b[0m\n",
      "\u001b[0;34m    dataset = dataset.repeat(2)\u001b[0m\n",
      "\u001b[0;34m    # [1, 0, 2, 1, 2, 0]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    dataset = tf.data.Dataset.range(3)\u001b[0m\n",
      "\u001b[0;34m    dataset = dataset.shuffle(3, reshuffle_each_iteration=False)\u001b[0m\n",
      "\u001b[0;34m    dataset = dataset.repeat(2)\u001b[0m\n",
      "\u001b[0;34m    # [1, 0, 2, 1, 0, 2]\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    In TF 2.0, `tf.data.Dataset` objects are Python iterables which makes it\u001b[0m\n",
      "\u001b[0;34m    possible to also create epochs through Python iteration:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```python\u001b[0m\n",
      "\u001b[0;34m    dataset = tf.data.Dataset.range(3)\u001b[0m\n",
      "\u001b[0;34m    dataset = dataset.shuffle(3, reshuffle_each_iteration=True)\u001b[0m\n",
      "\u001b[0;34m    list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    # [1, 0, 2]\u001b[0m\n",
      "\u001b[0;34m    list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    # [1, 2, 0]\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```python\u001b[0m\n",
      "\u001b[0;34m    dataset = tf.data.Dataset.range(3)\u001b[0m\n",
      "\u001b[0;34m    dataset = dataset.shuffle(3, reshuffle_each_iteration=False)\u001b[0m\n",
      "\u001b[0;34m    list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    # [1, 0, 2]\u001b[0m\n",
      "\u001b[0;34m    list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    # [1, 0, 2]\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    #### Fully shuffling all the data\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    To shuffle an entire dataset, set `buffer_size=dataset.cardinality()`. This\u001b[0m\n",
      "\u001b[0;34m    is equivalent to setting the `buffer_size` equal to the number of elements\u001b[0m\n",
      "\u001b[0;34m    in the dataset, resulting in uniform shuffle.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Note: `shuffle(dataset.cardinality())` loads the full dataset into memory so\u001b[0m\n",
      "\u001b[0;34m    that it can be shuffled. This will cause a memory overflow (OOM) error if\u001b[0m\n",
      "\u001b[0;34m    the dataset is too large, so full-shuffle should only be used for datasets\u001b[0m\n",
      "\u001b[0;34m    that are known to fit in the memory, such as datasets of filenames or other\u001b[0m\n",
      "\u001b[0;34m    small datasets.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```python\u001b[0m\n",
      "\u001b[0;34m    dataset = tf.data.Dataset.range(20)\u001b[0m\n",
      "\u001b[0;34m    dataset = dataset.shuffle(dataset.cardinality())\u001b[0m\n",
      "\u001b[0;34m    # [18, 4, 9, 2, 17, 8, 5, 10, 0, 6, 16, 3, 19, 7, 14, 11, 15, 13, 12, 1]\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      buffer_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\u001b[0m\n",
      "\u001b[0;34m        elements from this dataset from which the new dataset will sample. To\u001b[0m\n",
      "\u001b[0;34m        uniformly shuffle the entire dataset, use\u001b[0m\n",
      "\u001b[0;34m        `buffer_size=dataset.cardinality()`.\u001b[0m\n",
      "\u001b[0;34m      seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the random\u001b[0m\n",
      "\u001b[0;34m        seed that will be used to create the distribution. See\u001b[0m\n",
      "\u001b[0;34m        `tf.random.set_seed` for behavior.\u001b[0m\n",
      "\u001b[0;34m      reshuffle_each_iteration: (Optional.) A boolean, which if true indicates\u001b[0m\n",
      "\u001b[0;34m        that the dataset should be pseudorandomly reshuffled each time it is\u001b[0m\n",
      "\u001b[0;34m        iterated over. (Defaults to `True`.)\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mshuffle_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shuffle\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshuffle_each_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Caches the elements in this dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The first time the dataset is iterated over, its elements will be cached\u001b[0m\n",
      "\u001b[0;34m    either in the specified file or in memory. Subsequent iterations will\u001b[0m\n",
      "\u001b[0;34m    use the cached data.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Note: To guarantee that the cache gets finalized, the input dataset must be\u001b[0m\n",
      "\u001b[0;34m    iterated through in its entirety, until it raises StopIteration. Otherwise,\u001b[0m\n",
      "\u001b[0;34m    subsequent iterations may not use cached data.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.range(5)\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.map(lambda x: x**2)\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.cache()\u001b[0m\n",
      "\u001b[0;34m    >>> # The first time reading through the data will generate the data using\u001b[0m\n",
      "\u001b[0;34m    >>> # `range` and `map`.\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [0, 1, 4, 9, 16]\u001b[0m\n",
      "\u001b[0;34m    >>> # Subsequent iterations read from the cache.\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [0, 1, 4, 9, 16]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    When caching to a file, the cached data will persist across runs. Even the\u001b[0m\n",
      "\u001b[0;34m    first iteration through the data will read from the cache file. Changing\u001b[0m\n",
      "\u001b[0;34m    the input pipeline before the call to `.cache()` will have no effect until\u001b[0m\n",
      "\u001b[0;34m    the cache file is removed or the filename is changed.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```python\u001b[0m\n",
      "\u001b[0;34m    dataset = tf.data.Dataset.range(5)\u001b[0m\n",
      "\u001b[0;34m    dataset = dataset.cache(\"/path/to/file\")\u001b[0m\n",
      "\u001b[0;34m    list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    # [0, 1, 2, 3, 4]\u001b[0m\n",
      "\u001b[0;34m    dataset = tf.data.Dataset.range(10)\u001b[0m\n",
      "\u001b[0;34m    dataset = dataset.cache(\"/path/to/file\")  # Same file!\u001b[0m\n",
      "\u001b[0;34m    list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    # [0, 1, 2, 3, 4]\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Note: `cache` will produce exactly the same elements during each iteration\u001b[0m\n",
      "\u001b[0;34m    through the dataset. If you wish to randomize the iteration order, make sure\u001b[0m\n",
      "\u001b[0;34m    to call `shuffle` *after* calling `cache`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      filename: A `tf.string` scalar `tf.Tensor`, representing the name of a\u001b[0m\n",
      "\u001b[0;34m        directory on the filesystem to use for caching elements in this Dataset.\u001b[0m\n",
      "\u001b[0;34m        If a filename is not provided, the dataset will be cached in memory.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops -> cache_op ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcache_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mcache_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Creates a `Dataset` with at most `count` elements from this dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.range(10)\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.take(3)\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [0, 1, 2]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      count: A `tf.int64` scalar `tf.Tensor`, representing the number of\u001b[0m\n",
      "\u001b[0;34m        elements of this dataset that should be taken to form the new dataset.\u001b[0m\n",
      "\u001b[0;34m        If `count` is -1, or if `count` is greater than the size of this\u001b[0m\n",
      "\u001b[0;34m        dataset, the new dataset will contain all elements of this dataset.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# take_op -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtake_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mtake_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Creates a `Dataset` that skips `count` elements from this dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.range(10)\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.skip(7)\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [7, 8, 9]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      count: A `tf.int64` scalar `tf.Tensor`, representing the number of\u001b[0m\n",
      "\u001b[0;34m        elements of this dataset that should be skipped to form the new dataset.\u001b[0m\n",
      "\u001b[0;34m        If `count` is greater than the size of this dataset, the new dataset\u001b[0m\n",
      "\u001b[0;34m        will contain no elements.  If `count` is -1, skips the entire dataset.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# skip_op -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mskip_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mskip_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mshard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_shards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Creates a `Dataset` that includes only 1/`num_shards` of this dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    `shard` is deterministic. The Dataset produced by `A.shard(n, i)` will\u001b[0m\n",
      "\u001b[0;34m    contain all elements of A whose index mod n = i.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> A = tf.data.Dataset.range(10)\u001b[0m\n",
      "\u001b[0;34m    >>> B = A.shard(num_shards=3, index=0)\u001b[0m\n",
      "\u001b[0;34m    >>> list(B.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [0, 3, 6, 9]\u001b[0m\n",
      "\u001b[0;34m    >>> C = A.shard(num_shards=3, index=1)\u001b[0m\n",
      "\u001b[0;34m    >>> list(C.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [1, 4, 7]\u001b[0m\n",
      "\u001b[0;34m    >>> D = A.shard(num_shards=3, index=2)\u001b[0m\n",
      "\u001b[0;34m    >>> list(D.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [2, 5, 8]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This dataset operator is very useful when running distributed training, as\u001b[0m\n",
      "\u001b[0;34m    it allows each worker to read a unique subset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    When reading a single input file, you can shard elements as follows:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```python\u001b[0m\n",
      "\u001b[0;34m    d = tf.data.TFRecordDataset(input_file)\u001b[0m\n",
      "\u001b[0;34m    d = d.shard(num_workers, worker_index)\u001b[0m\n",
      "\u001b[0;34m    d = d.repeat(num_epochs)\u001b[0m\n",
      "\u001b[0;34m    d = d.shuffle(shuffle_buffer_size)\u001b[0m\n",
      "\u001b[0;34m    d = d.map(parser_fn, num_parallel_calls=num_map_threads)\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Important caveats:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    - Be sure to shard before you use any randomizing operator (such as\u001b[0m\n",
      "\u001b[0;34m      shuffle).\u001b[0m\n",
      "\u001b[0;34m    - Generally it is best if the shard operator is used early in the dataset\u001b[0m\n",
      "\u001b[0;34m      pipeline. For example, when reading from a set of TFRecord files, shard\u001b[0m\n",
      "\u001b[0;34m      before converting the dataset to input samples. This avoids reading every\u001b[0m\n",
      "\u001b[0;34m      file on every worker. The following is an example of an efficient\u001b[0m\n",
      "\u001b[0;34m      sharding strategy within a complete pipeline:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```python\u001b[0m\n",
      "\u001b[0;34m    d = Dataset.list_files(pattern, shuffle=False)\u001b[0m\n",
      "\u001b[0;34m    d = d.shard(num_workers, worker_index)\u001b[0m\n",
      "\u001b[0;34m    d = d.repeat(num_epochs)\u001b[0m\n",
      "\u001b[0;34m    d = d.shuffle(shuffle_buffer_size)\u001b[0m\n",
      "\u001b[0;34m    d = d.interleave(tf.data.TFRecordDataset,\u001b[0m\n",
      "\u001b[0;34m                     cycle_length=num_readers, block_length=1)\u001b[0m\n",
      "\u001b[0;34m    d = d.map(parser_fn, num_parallel_calls=num_map_threads)\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      num_shards: A `tf.int64` scalar `tf.Tensor`, representing the number of\u001b[0m\n",
      "\u001b[0;34m        shards operating in parallel.\u001b[0m\n",
      "\u001b[0;34m      index: A `tf.int64` scalar `tf.Tensor`, representing the worker index.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Raises:\u001b[0m\n",
      "\u001b[0;34m      InvalidArgumentError: if `num_shards` or `index` are illegal values.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m        Note: error checking is done on a best-effort basis, and errors aren't\u001b[0m\n",
      "\u001b[0;34m        guaranteed to be caught upon dataset creation. (e.g. providing in a\u001b[0m\n",
      "\u001b[0;34m        placeholder tensor bypasses the early checking, and will instead result\u001b[0m\n",
      "\u001b[0;34m        in an error during a session.run call.)\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshard_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mshard_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_shards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m           \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m           \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m           \u001b[0mshard_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m           \u001b[0mcheckpoint_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Saves the content of the given dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m      Example usage:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m      >>> import tempfile\u001b[0m\n",
      "\u001b[0;34m      >>> path = os.path.join(tempfile.gettempdir(), \"saved_data\")\u001b[0m\n",
      "\u001b[0;34m      >>> # Save a dataset\u001b[0m\n",
      "\u001b[0;34m      >>> dataset = tf.data.Dataset.range(2)\u001b[0m\n",
      "\u001b[0;34m      >>> dataset.save(path)\u001b[0m\n",
      "\u001b[0;34m      >>> new_dataset = tf.data.Dataset.load(path)\u001b[0m\n",
      "\u001b[0;34m      >>> for elem in new_dataset:\u001b[0m\n",
      "\u001b[0;34m      ...   print(elem)\u001b[0m\n",
      "\u001b[0;34m      tf.Tensor(0, shape=(), dtype=int64)\u001b[0m\n",
      "\u001b[0;34m      tf.Tensor(1, shape=(), dtype=int64)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m      The saved dataset is saved in multiple file \"shards\". By default, the\u001b[0m\n",
      "\u001b[0;34m      dataset output is divided to shards in a round-robin fashion but custom\u001b[0m\n",
      "\u001b[0;34m      sharding can be specified via the `shard_func` function. For example, you\u001b[0m\n",
      "\u001b[0;34m      can save the dataset to using a single shard as follows:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m      ```python\u001b[0m\n",
      "\u001b[0;34m      dataset = make_dataset()\u001b[0m\n",
      "\u001b[0;34m      def custom_shard_func(element):\u001b[0m\n",
      "\u001b[0;34m        return np.int64(0)\u001b[0m\n",
      "\u001b[0;34m      dataset.save(\u001b[0m\n",
      "\u001b[0;34m          path=\"/path/to/data\", ..., shard_func=custom_shard_func)\u001b[0m\n",
      "\u001b[0;34m      ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m      To enable checkpointing, pass in `checkpoint_args` to the `save` method\u001b[0m\n",
      "\u001b[0;34m      as follows:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m      ```python\u001b[0m\n",
      "\u001b[0;34m      dataset = tf.data.Dataset.range(100)\u001b[0m\n",
      "\u001b[0;34m      save_dir = \"...\"\u001b[0m\n",
      "\u001b[0;34m      checkpoint_prefix = \"...\"\u001b[0m\n",
      "\u001b[0;34m      step_counter = tf.Variable(0, trainable=False)\u001b[0m\n",
      "\u001b[0;34m      checkpoint_args = {\u001b[0m\n",
      "\u001b[0;34m        \"checkpoint_interval\": 50,\u001b[0m\n",
      "\u001b[0;34m        \"step_counter\": step_counter,\u001b[0m\n",
      "\u001b[0;34m        \"directory\": checkpoint_prefix,\u001b[0m\n",
      "\u001b[0;34m        \"max_to_keep\": 20,\u001b[0m\n",
      "\u001b[0;34m      }\u001b[0m\n",
      "\u001b[0;34m      dataset.save(dataset, save_dir, checkpoint_args=checkpoint_args)\u001b[0m\n",
      "\u001b[0;34m      ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m      NOTE: The directory layout and file format used for saving the dataset is\u001b[0m\n",
      "\u001b[0;34m      considered an implementation detail and may change. For this reason,\u001b[0m\n",
      "\u001b[0;34m      datasets saved through `tf.data.Dataset.save` should only be consumed\u001b[0m\n",
      "\u001b[0;34m      through `tf.data.Dataset.load`, which is guaranteed to be\u001b[0m\n",
      "\u001b[0;34m      backwards compatible.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m     path: Required. A directory to use for saving the dataset.\u001b[0m\n",
      "\u001b[0;34m     compression: Optional. The algorithm to use to compress data when writing\u001b[0m\n",
      "\u001b[0;34m          it. Supported options are `GZIP` and `NONE`. Defaults to `NONE`.\u001b[0m\n",
      "\u001b[0;34m     shard_func: Optional. A function to control the mapping of dataset\u001b[0m\n",
      "\u001b[0;34m          elements to file shards. The function is expected to map elements of\u001b[0m\n",
      "\u001b[0;34m          the input dataset to int64 shard IDs. If present, the function will be\u001b[0m\n",
      "\u001b[0;34m          traced and executed as graph computation.\u001b[0m\n",
      "\u001b[0;34m     checkpoint_args: Optional args for checkpointing which will be passed into\u001b[0m\n",
      "\u001b[0;34m          the `tf.train.CheckpointManager`. If `checkpoint_args` are not\u001b[0m\n",
      "\u001b[0;34m          specified, then checkpointing will not be performed. The `save()`\u001b[0m\n",
      "\u001b[0;34m          implementation creates a `tf.train.Checkpoint` object internally, so\u001b[0m\n",
      "\u001b[0;34m          users should not set the `checkpoint` argument in `checkpoint_args`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      An operation which when executed performs the save. When writing\u001b[0m\n",
      "\u001b[0;34m      checkpoints, returns None. The return value is useful in unit tests.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Raises:\u001b[0m\n",
      "\u001b[0;34m      ValueError if `checkpoint` is passed into `checkpoint_args`.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops -> save_op ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0msave_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Loads a previously saved dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Example usage:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> import tempfile\u001b[0m\n",
      "\u001b[0;34m    >>> path = os.path.join(tempfile.gettempdir(), \"saved_data\")\u001b[0m\n",
      "\u001b[0;34m    >>> # Save a dataset\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.range(2)\u001b[0m\n",
      "\u001b[0;34m    >>> tf.data.Dataset.save(dataset, path)\u001b[0m\n",
      "\u001b[0;34m    >>> new_dataset = tf.data.Dataset.load(path)\u001b[0m\n",
      "\u001b[0;34m    >>> for elem in new_dataset:\u001b[0m\n",
      "\u001b[0;34m    ...   print(elem)\u001b[0m\n",
      "\u001b[0;34m    tf.Tensor(0, shape=(), dtype=int64)\u001b[0m\n",
      "\u001b[0;34m    tf.Tensor(1, shape=(), dtype=int64)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    If the default option of sharding the saved dataset was used, the element\u001b[0m\n",
      "\u001b[0;34m    order of the saved dataset will be preserved when loading it.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The `reader_func` argument can be used to specify a custom order in which\u001b[0m\n",
      "\u001b[0;34m    elements should be loaded from the individual shards. The `reader_func` is\u001b[0m\n",
      "\u001b[0;34m    expected to take a single argument -- a dataset of datasets, each containing\u001b[0m\n",
      "\u001b[0;34m    elements of one of the shards -- and return a dataset of elements. For\u001b[0m\n",
      "\u001b[0;34m    example, the order of shards can be shuffled when loading them as follows:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```python\u001b[0m\n",
      "\u001b[0;34m    def custom_reader_func(datasets):\u001b[0m\n",
      "\u001b[0;34m      datasets = datasets.shuffle(NUM_SHARDS)\u001b[0m\n",
      "\u001b[0;34m      return datasets.interleave(lambda x: x, num_parallel_calls=AUTOTUNE)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    dataset = tf.data.Dataset.load(\u001b[0m\n",
      "\u001b[0;34m        path=\"/path/to/data\", ..., reader_func=custom_reader_func)\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      path: Required. A path pointing to a previously saved dataset.\u001b[0m\n",
      "\u001b[0;34m      element_spec: Optional. A nested structure of `tf.TypeSpec` objects\u001b[0m\n",
      "\u001b[0;34m        matching the structure of an element of the saved dataset and specifying\u001b[0m\n",
      "\u001b[0;34m        the type of individual element components. If not provided, the nested\u001b[0m\n",
      "\u001b[0;34m        structure of `tf.TypeSpec` saved with the saved dataset is used. Note\u001b[0m\n",
      "\u001b[0;34m        that this argument is required in graph mode.\u001b[0m\n",
      "\u001b[0;34m      compression: Optional. The algorithm to use to decompress the data when\u001b[0m\n",
      "\u001b[0;34m        reading it. Supported options are `GZIP` and `NONE`. Defaults to `NONE`.\u001b[0m\n",
      "\u001b[0;34m      reader_func: Optional. A function to control how to read data from shards.\u001b[0m\n",
      "\u001b[0;34m        If present, the function will be traced and executed as graph\u001b[0m\n",
      "\u001b[0;34m        computation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A `tf.data.Dataset` instance.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Raises:\u001b[0m\n",
      "\u001b[0;34m      FileNotFoundError: If `element_spec` is not specified and the saved nested\u001b[0m\n",
      "\u001b[0;34m        structure of `tf.TypeSpec` can not be located with the saved dataset.\u001b[0m\n",
      "\u001b[0;34m      ValueError: If `element_spec` is not specified and the method is executed\u001b[0m\n",
      "\u001b[0;34m        in graph mode.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops -> load_op ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mload_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0melement_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreader_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreader_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Combines consecutive elements of this dataset into batches.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.range(8)\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.batch(3)\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [array([0, 1, 2]), array([3, 4, 5]), array([6, 7])]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.range(8)\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.batch(3, drop_remainder=True)\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [array([0, 1, 2]), array([3, 4, 5])]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The components of the resulting element will have an additional outer\u001b[0m\n",
      "\u001b[0;34m    dimension, which will be `batch_size` (or `N % batch_size` for the last\u001b[0m\n",
      "\u001b[0;34m    element if `batch_size` does not divide the number of input elements `N`\u001b[0m\n",
      "\u001b[0;34m    evenly and `drop_remainder` is `False`). If your program depends on the\u001b[0m\n",
      "\u001b[0;34m    batches having the same outer dimension, you should set the `drop_remainder`\u001b[0m\n",
      "\u001b[0;34m    argument to `True` to prevent the smaller batch from being produced.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Note: If your program requires data to have a statically known shape (e.g.,\u001b[0m\n",
      "\u001b[0;34m    when using XLA), you should use `drop_remainder=True`. Without\u001b[0m\n",
      "\u001b[0;34m    `drop_remainder=True` the shape of the output dataset will have an unknown\u001b[0m\n",
      "\u001b[0;34m    leading dimension due to the possibility of a smaller final batch.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\u001b[0m\n",
      "\u001b[0;34m        consecutive elements of this dataset to combine in a single batch.\u001b[0m\n",
      "\u001b[0;34m      drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\u001b[0m\n",
      "\u001b[0;34m        whether the last batch should be dropped in the case it has fewer than\u001b[0m\n",
      "\u001b[0;34m        `batch_size` elements; the default behavior is not to drop the smaller\u001b[0m\n",
      "\u001b[0;34m        batch.\u001b[0m\n",
      "\u001b[0;34m      num_parallel_calls: (Optional.) A `tf.int64` scalar `tf.Tensor`,\u001b[0m\n",
      "\u001b[0;34m        representing the number of batches to compute asynchronously in\u001b[0m\n",
      "\u001b[0;34m        parallel.\u001b[0m\n",
      "\u001b[0;34m        If not specified, batches will be computed sequentially. If the value\u001b[0m\n",
      "\u001b[0;34m        `tf.data.AUTOTUNE` is used, then the number of parallel\u001b[0m\n",
      "\u001b[0;34m        calls is set dynamically based on available resources.\u001b[0m\n",
      "\u001b[0;34m      deterministic: (Optional.) When `num_parallel_calls` is specified, if this\u001b[0m\n",
      "\u001b[0;34m        boolean is specified (`True` or `False`), it controls the order in which\u001b[0m\n",
      "\u001b[0;34m        the transformation produces elements. If set to `False`, the\u001b[0m\n",
      "\u001b[0;34m        transformation is allowed to yield elements out of order to trade\u001b[0m\n",
      "\u001b[0;34m        determinism for performance. If not specified, the\u001b[0m\n",
      "\u001b[0;34m        `tf.data.Options.deterministic` option (`True` by default) controls the\u001b[0m\n",
      "\u001b[0;34m        behavior.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops -> batch_op ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access,redefined-outer-name\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbatch_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mbatch_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                           \u001b[0mdeterministic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access,redefined-outer-name\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mpadded_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mpadded_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mpadding_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Combines consecutive elements of this dataset into padded batches.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This transformation combines multiple consecutive elements of the input\u001b[0m\n",
      "\u001b[0;34m    dataset into a single element.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Like `tf.data.Dataset.batch`, the components of the resulting element will\u001b[0m\n",
      "\u001b[0;34m    have an additional outer dimension, which will be `batch_size` (or\u001b[0m\n",
      "\u001b[0;34m    `N % batch_size` for the last element if `batch_size` does not divide the\u001b[0m\n",
      "\u001b[0;34m    number of input elements `N` evenly and `drop_remainder` is `False`). If\u001b[0m\n",
      "\u001b[0;34m    your program depends on the batches having the same outer dimension, you\u001b[0m\n",
      "\u001b[0;34m    should set the `drop_remainder` argument to `True` to prevent the smaller\u001b[0m\n",
      "\u001b[0;34m    batch from being produced.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Unlike `tf.data.Dataset.batch`, the input elements to be batched may have\u001b[0m\n",
      "\u001b[0;34m    different shapes, and this transformation will pad each component to the\u001b[0m\n",
      "\u001b[0;34m    respective shape in `padded_shapes`. The `padded_shapes` argument\u001b[0m\n",
      "\u001b[0;34m    determines the resulting shape for each dimension of each component in an\u001b[0m\n",
      "\u001b[0;34m    output element:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    * If the dimension is a constant, the component will be padded out to that\u001b[0m\n",
      "\u001b[0;34m      length in that dimension.\u001b[0m\n",
      "\u001b[0;34m    * If the dimension is unknown, the component will be padded out to the\u001b[0m\n",
      "\u001b[0;34m      maximum length of all elements in that dimension.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> A = (tf.data.Dataset\u001b[0m\n",
      "\u001b[0;34m    ...      .range(1, 5, output_type=tf.int32)\u001b[0m\n",
      "\u001b[0;34m    ...      .map(lambda x: tf.fill([x], x)))\u001b[0m\n",
      "\u001b[0;34m    >>> # Pad to the smallest per-batch size that fits all elements.\u001b[0m\n",
      "\u001b[0;34m    >>> B = A.padded_batch(2)\u001b[0m\n",
      "\u001b[0;34m    >>> for element in B.as_numpy_iterator():\u001b[0m\n",
      "\u001b[0;34m    ...   print(element)\u001b[0m\n",
      "\u001b[0;34m    [[1 0]\u001b[0m\n",
      "\u001b[0;34m     [2 2]]\u001b[0m\n",
      "\u001b[0;34m    [[3 3 3 0]\u001b[0m\n",
      "\u001b[0;34m     [4 4 4 4]]\u001b[0m\n",
      "\u001b[0;34m    >>> # Pad to a fixed size.\u001b[0m\n",
      "\u001b[0;34m    >>> C = A.padded_batch(2, padded_shapes=5)\u001b[0m\n",
      "\u001b[0;34m    >>> for element in C.as_numpy_iterator():\u001b[0m\n",
      "\u001b[0;34m    ...   print(element)\u001b[0m\n",
      "\u001b[0;34m    [[1 0 0 0 0]\u001b[0m\n",
      "\u001b[0;34m     [2 2 0 0 0]]\u001b[0m\n",
      "\u001b[0;34m    [[3 3 3 0 0]\u001b[0m\n",
      "\u001b[0;34m     [4 4 4 4 0]]\u001b[0m\n",
      "\u001b[0;34m    >>> # Pad with a custom value.\u001b[0m\n",
      "\u001b[0;34m    >>> D = A.padded_batch(2, padded_shapes=5, padding_values=-1)\u001b[0m\n",
      "\u001b[0;34m    >>> for element in D.as_numpy_iterator():\u001b[0m\n",
      "\u001b[0;34m    ...   print(element)\u001b[0m\n",
      "\u001b[0;34m    [[ 1 -1 -1 -1 -1]\u001b[0m\n",
      "\u001b[0;34m     [ 2  2 -1 -1 -1]]\u001b[0m\n",
      "\u001b[0;34m    [[ 3  3  3 -1 -1]\u001b[0m\n",
      "\u001b[0;34m     [ 4  4  4  4 -1]]\u001b[0m\n",
      "\u001b[0;34m    >>> # Components of nested elements can be padded independently.\u001b[0m\n",
      "\u001b[0;34m    >>> elements = [([1, 2, 3], [10]),\u001b[0m\n",
      "\u001b[0;34m    ...             ([4, 5], [11, 12])]\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_generator(\u001b[0m\n",
      "\u001b[0;34m    ...     lambda: iter(elements), (tf.int32, tf.int32))\u001b[0m\n",
      "\u001b[0;34m    >>> # Pad the first component of the tuple to length 4, and the second\u001b[0m\n",
      "\u001b[0;34m    >>> # component to the smallest size that fits.\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.padded_batch(2,\u001b[0m\n",
      "\u001b[0;34m    ...     padded_shapes=([4], [None]),\u001b[0m\n",
      "\u001b[0;34m    ...     padding_values=(-1, 100))\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [(array([[ 1,  2,  3, -1], [ 4,  5, -1, -1]], dtype=int32),\u001b[0m\n",
      "\u001b[0;34m      array([[ 10, 100], [ 11,  12]], dtype=int32))]\u001b[0m\n",
      "\u001b[0;34m    >>> # Pad with a single value and multiple components.\u001b[0m\n",
      "\u001b[0;34m    >>> E = tf.data.Dataset.zip((A, A)).padded_batch(2, padding_values=-1)\u001b[0m\n",
      "\u001b[0;34m    >>> for element in E.as_numpy_iterator():\u001b[0m\n",
      "\u001b[0;34m    ...   print(element)\u001b[0m\n",
      "\u001b[0;34m    (array([[ 1, -1],\u001b[0m\n",
      "\u001b[0;34m           [ 2,  2]], dtype=int32), array([[ 1, -1],\u001b[0m\n",
      "\u001b[0;34m           [ 2,  2]], dtype=int32))\u001b[0m\n",
      "\u001b[0;34m    (array([[ 3,  3,  3, -1],\u001b[0m\n",
      "\u001b[0;34m           [ 4,  4,  4,  4]], dtype=int32), array([[ 3,  3,  3, -1],\u001b[0m\n",
      "\u001b[0;34m           [ 4,  4,  4,  4]], dtype=int32))\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    See also `tf.data.experimental.dense_to_sparse_batch`, which combines\u001b[0m\n",
      "\u001b[0;34m    elements that may have different shapes into a `tf.sparse.SparseTensor`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\u001b[0m\n",
      "\u001b[0;34m        consecutive elements of this dataset to combine in a single batch.\u001b[0m\n",
      "\u001b[0;34m      padded_shapes: (Optional.) A (nested) structure of `tf.TensorShape` or\u001b[0m\n",
      "\u001b[0;34m        `tf.int64` vector tensor-like objects representing the shape to which\u001b[0m\n",
      "\u001b[0;34m        the respective component of each input element should be padded prior\u001b[0m\n",
      "\u001b[0;34m        to batching. Any unknown dimensions will be padded to the maximum size\u001b[0m\n",
      "\u001b[0;34m        of that dimension in each batch. If unset, all dimensions of all\u001b[0m\n",
      "\u001b[0;34m        components are padded to the maximum size in the batch. `padded_shapes`\u001b[0m\n",
      "\u001b[0;34m        must be set if any component has an unknown rank.\u001b[0m\n",
      "\u001b[0;34m      padding_values: (Optional.) A (nested) structure of scalar-shaped\u001b[0m\n",
      "\u001b[0;34m        `tf.Tensor`, representing the padding values to use for the respective\u001b[0m\n",
      "\u001b[0;34m        components. None represents that the (nested) structure should be padded\u001b[0m\n",
      "\u001b[0;34m        with default values.  Defaults are `0` for numeric types and the empty\u001b[0m\n",
      "\u001b[0;34m        string for string types. The `padding_values` should have the same\u001b[0m\n",
      "\u001b[0;34m        (nested) structure as the input dataset. If `padding_values` is a single\u001b[0m\n",
      "\u001b[0;34m        element and the input dataset has multiple components, then the same\u001b[0m\n",
      "\u001b[0;34m        `padding_values` will be used to pad every component of the dataset.\u001b[0m\n",
      "\u001b[0;34m        If `padding_values` is a scalar, then its value will be broadcasted\u001b[0m\n",
      "\u001b[0;34m        to match the shape of each component.\u001b[0m\n",
      "\u001b[0;34m      drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\u001b[0m\n",
      "\u001b[0;34m        whether the last batch should be dropped in the case it has fewer than\u001b[0m\n",
      "\u001b[0;34m        `batch_size` elements; the default behavior is not to drop the smaller\u001b[0m\n",
      "\u001b[0;34m        batch.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Raises:\u001b[0m\n",
      "\u001b[0;34m      ValueError: If a component has an unknown rank, and the `padded_shapes`\u001b[0m\n",
      "\u001b[0;34m        argument is not set.\u001b[0m\n",
      "\u001b[0;34m      TypeError: If a component is of an unsupported type. The list of supported\u001b[0m\n",
      "\u001b[0;34m        types is documented in\u001b[0m\n",
      "\u001b[0;34m        https://www.tensorflow.org/guide/data#dataset_structure.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# padded_batch_op -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpadded_batch_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mpadded_batch_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_padded_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                         \u001b[0mpadding_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mragged_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mrow_splits_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Combines consecutive elements of this dataset into `tf.RaggedTensor`s.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Like `tf.data.Dataset.batch`, the components of the resulting element will\u001b[0m\n",
      "\u001b[0;34m    have an additional outer dimension, which will be `batch_size` (or\u001b[0m\n",
      "\u001b[0;34m    `N % batch_size` for the last element if `batch_size` does not divide the\u001b[0m\n",
      "\u001b[0;34m    number of input elements `N` evenly and `drop_remainder` is `False`). If\u001b[0m\n",
      "\u001b[0;34m    your program depends on the batches having the same outer dimension, you\u001b[0m\n",
      "\u001b[0;34m    should set the `drop_remainder` argument to `True` to prevent the smaller\u001b[0m\n",
      "\u001b[0;34m    batch from being produced.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Unlike `tf.data.Dataset.batch`, the input elements to be batched may have\u001b[0m\n",
      "\u001b[0;34m    different shapes:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    *  If an input element is a `tf.Tensor` whose static `tf.TensorShape` is\u001b[0m\n",
      "\u001b[0;34m    fully defined, then it is batched as normal.\u001b[0m\n",
      "\u001b[0;34m    *  If an input element is a `tf.Tensor` whose static `tf.TensorShape`\u001b[0m\n",
      "\u001b[0;34m    contains one or more axes with unknown size (i.e., `shape[i]=None`), then\u001b[0m\n",
      "\u001b[0;34m    the output will contain a `tf.RaggedTensor` that is ragged up to any of such\u001b[0m\n",
      "\u001b[0;34m    dimensions.\u001b[0m\n",
      "\u001b[0;34m    *  If an input element is a `tf.RaggedTensor` or any other type, then it is\u001b[0m\n",
      "\u001b[0;34m    batched as normal.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Example:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.range(6)\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.map(lambda x: tf.range(x))\u001b[0m\n",
      "\u001b[0;34m    >>> dataset.element_spec.shape\u001b[0m\n",
      "\u001b[0;34m    TensorShape([None])\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.ragged_batch(2)\u001b[0m\n",
      "\u001b[0;34m    >>> for batch in dataset:\u001b[0m\n",
      "\u001b[0;34m    ...   print(batch)\u001b[0m\n",
      "\u001b[0;34m    <tf.RaggedTensor [[], [0]]>\u001b[0m\n",
      "\u001b[0;34m    <tf.RaggedTensor [[0, 1], [0, 1, 2]]>\u001b[0m\n",
      "\u001b[0;34m    <tf.RaggedTensor [[0, 1, 2, 3], [0, 1, 2, 3, 4]]>\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\u001b[0m\n",
      "\u001b[0;34m        consecutive elements of this dataset to combine in a single batch.\u001b[0m\n",
      "\u001b[0;34m      drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\u001b[0m\n",
      "\u001b[0;34m        whether the last batch should be dropped in the case it has fewer than\u001b[0m\n",
      "\u001b[0;34m        `batch_size` elements; the default behavior is not to drop the smaller\u001b[0m\n",
      "\u001b[0;34m        batch.\u001b[0m\n",
      "\u001b[0;34m      row_splits_dtype: The dtype that should be used for the `row_splits` of\u001b[0m\n",
      "\u001b[0;34m        any new ragged tensors.  Existing `tf.RaggedTensor` elements do not have\u001b[0m\n",
      "\u001b[0;34m        their row_splits dtype changed.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A string indicating a name for the `tf.data` operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# ragged_batch_op -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mragged_batch_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mragged_batch_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ragged_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                         \u001b[0mrow_splits_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0msparse_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Combines consecutive elements into `tf.sparse.SparseTensor`s.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Like `Dataset.padded_batch()`, this transformation combines multiple\u001b[0m\n",
      "\u001b[0;34m    consecutive elements of the dataset, which might have different\u001b[0m\n",
      "\u001b[0;34m    shapes, into a single element. The resulting element has three\u001b[0m\n",
      "\u001b[0;34m    components (`indices`, `values`, and `dense_shape`), which\u001b[0m\n",
      "\u001b[0;34m    comprise a `tf.sparse.SparseTensor` that represents the same data. The\u001b[0m\n",
      "\u001b[0;34m    `row_shape` represents the dense shape of each row in the\u001b[0m\n",
      "\u001b[0;34m    resulting `tf.sparse.SparseTensor`, to which the effective batch size is\u001b[0m\n",
      "\u001b[0;34m    prepended. For example:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```python\u001b[0m\n",
      "\u001b[0;34m    # NOTE: The following examples use `{ ... }` to represent the\u001b[0m\n",
      "\u001b[0;34m    # contents of a dataset.\u001b[0m\n",
      "\u001b[0;34m    a = { ['a', 'b', 'c'], ['a', 'b'], ['a', 'b', 'c', 'd'] }\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    a.apply(tf.data.experimental.dense_to_sparse_batch(\u001b[0m\n",
      "\u001b[0;34m        batch_size=2, row_shape=[6])) ==\u001b[0m\n",
      "\u001b[0;34m    {\u001b[0m\n",
      "\u001b[0;34m        ([[0, 0], [0, 1], [0, 2], [1, 0], [1, 1]],  # indices\u001b[0m\n",
      "\u001b[0;34m         ['a', 'b', 'c', 'a', 'b'],                 # values\u001b[0m\n",
      "\u001b[0;34m         [2, 6]),                                   # dense_shape\u001b[0m\n",
      "\u001b[0;34m        ([[0, 0], [0, 1], [0, 2], [0, 3]],\u001b[0m\n",
      "\u001b[0;34m         ['a', 'b', 'c', 'd'],\u001b[0m\n",
      "\u001b[0;34m         [1, 6])\u001b[0m\n",
      "\u001b[0;34m    }\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\u001b[0m\n",
      "\u001b[0;34m        consecutive elements of this dataset to combine in a single batch.\u001b[0m\n",
      "\u001b[0;34m      row_shape: A `tf.TensorShape` or `tf.int64` vector tensor-like object\u001b[0m\n",
      "\u001b[0;34m        representing the equivalent dense shape of a row in the resulting\u001b[0m\n",
      "\u001b[0;34m        `tf.sparse.SparseTensor`. Each element of this dataset must have the\u001b[0m\n",
      "\u001b[0;34m        same rank as `row_shape`, and must have size less than or equal to\u001b[0m\n",
      "\u001b[0;34m        `row_shape` in each dimension.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A string indicating a name for the `tf.data` operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# sparse_batch_op -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse_batch_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0msparse_batch_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Maps `map_func` across the elements of this dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This transformation applies `map_func` to each element of this dataset, and\u001b[0m\n",
      "\u001b[0;34m    returns a new dataset containing the transformed elements, in the same\u001b[0m\n",
      "\u001b[0;34m    order as they appeared in the input. `map_func` can be used to change both\u001b[0m\n",
      "\u001b[0;34m    the values and the structure of a dataset's elements. Supported structure\u001b[0m\n",
      "\u001b[0;34m    constructs are documented\u001b[0m\n",
      "\u001b[0;34m    [here](https://www.tensorflow.org/guide/data#dataset_structure).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    For example, `map` can be used for adding 1 to each element, or projecting a\u001b[0m\n",
      "\u001b[0;34m    subset of element components.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.map(lambda x: x + 1)\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [2, 3, 4, 5, 6]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The input signature of `map_func` is determined by the structure of each\u001b[0m\n",
      "\u001b[0;34m    element in this dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = Dataset.range(5)\u001b[0m\n",
      "\u001b[0;34m    >>> # `map_func` takes a single argument of type `tf.Tensor` with the same\u001b[0m\n",
      "\u001b[0;34m    >>> # shape and dtype.\u001b[0m\n",
      "\u001b[0;34m    >>> result = dataset.map(lambda x: x + 1)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> # Each element is a tuple containing two `tf.Tensor` objects.\u001b[0m\n",
      "\u001b[0;34m    >>> elements = [(1, \"foo\"), (2, \"bar\"), (3, \"baz\")]\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_generator(\u001b[0m\n",
      "\u001b[0;34m    ...     lambda: elements, (tf.int32, tf.string))\u001b[0m\n",
      "\u001b[0;34m    >>> # `map_func` takes two arguments of type `tf.Tensor`. This function\u001b[0m\n",
      "\u001b[0;34m    >>> # projects out just the first component.\u001b[0m\n",
      "\u001b[0;34m    >>> result = dataset.map(lambda x_int, y_str: x_int)\u001b[0m\n",
      "\u001b[0;34m    >>> list(result.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [1, 2, 3]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> # Each element is a dictionary mapping strings to `tf.Tensor` objects.\u001b[0m\n",
      "\u001b[0;34m    >>> elements =  ([{\"a\": 1, \"b\": \"foo\"},\u001b[0m\n",
      "\u001b[0;34m    ...               {\"a\": 2, \"b\": \"bar\"},\u001b[0m\n",
      "\u001b[0;34m    ...               {\"a\": 3, \"b\": \"baz\"}])\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_generator(\u001b[0m\n",
      "\u001b[0;34m    ...     lambda: elements, {\"a\": tf.int32, \"b\": tf.string})\u001b[0m\n",
      "\u001b[0;34m    >>> # `map_func` takes a single argument of type `dict` with the same keys\u001b[0m\n",
      "\u001b[0;34m    >>> # as the elements.\u001b[0m\n",
      "\u001b[0;34m    >>> result = dataset.map(lambda d: str(d[\"a\"]) + d[\"b\"])\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The value or values returned by `map_func` determine the structure of each\u001b[0m\n",
      "\u001b[0;34m    element in the returned dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.range(3)\u001b[0m\n",
      "\u001b[0;34m    >>> # `map_func` returns two `tf.Tensor` objects.\u001b[0m\n",
      "\u001b[0;34m    >>> def g(x):\u001b[0m\n",
      "\u001b[0;34m    ...   return tf.constant(37.0), tf.constant([\"Foo\", \"Bar\", \"Baz\"])\u001b[0m\n",
      "\u001b[0;34m    >>> result = dataset.map(g)\u001b[0m\n",
      "\u001b[0;34m    >>> result.element_spec\u001b[0m\n",
      "\u001b[0;34m    (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(3,), \\\u001b[0m\n",
      "\u001b[0;34mdtype=tf.string, name=None))\u001b[0m\n",
      "\u001b[0;34m    >>> # Python primitives, lists, and NumPy arrays are implicitly converted to\u001b[0m\n",
      "\u001b[0;34m    >>> # `tf.Tensor`.\u001b[0m\n",
      "\u001b[0;34m    >>> def h(x):\u001b[0m\n",
      "\u001b[0;34m    ...   return 37.0, [\"Foo\", \"Bar\"], np.array([1.0, 2.0], dtype=np.float64)\u001b[0m\n",
      "\u001b[0;34m    >>> result = dataset.map(h)\u001b[0m\n",
      "\u001b[0;34m    >>> result.element_spec\u001b[0m\n",
      "\u001b[0;34m    (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(2,), \\\u001b[0m\n",
      "\u001b[0;34mdtype=tf.string, name=None), TensorSpec(shape=(2,), dtype=tf.float64, \\\u001b[0m\n",
      "\u001b[0;34mname=None))\u001b[0m\n",
      "\u001b[0;34m    >>> # `map_func` can return nested structures.\u001b[0m\n",
      "\u001b[0;34m    >>> def i(x):\u001b[0m\n",
      "\u001b[0;34m    ...   return (37.0, [42, 16]), \"foo\"\u001b[0m\n",
      "\u001b[0;34m    >>> result = dataset.map(i)\u001b[0m\n",
      "\u001b[0;34m    >>> result.element_spec\u001b[0m\n",
      "\u001b[0;34m    ((TensorSpec(shape=(), dtype=tf.float32, name=None),\u001b[0m\n",
      "\u001b[0;34m      TensorSpec(shape=(2,), dtype=tf.int32, name=None)),\u001b[0m\n",
      "\u001b[0;34m     TensorSpec(shape=(), dtype=tf.string, name=None))\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    `map_func` can accept as arguments and return any type of dataset element.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Note that irrespective of the context in which `map_func` is defined (eager\u001b[0m\n",
      "\u001b[0;34m    vs. graph), tf.data traces the function and executes it as a graph. To use\u001b[0m\n",
      "\u001b[0;34m    Python code inside of the function you have a few options:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    1) Rely on AutoGraph to convert Python code into an equivalent graph\u001b[0m\n",
      "\u001b[0;34m    computation. The downside of this approach is that AutoGraph can convert\u001b[0m\n",
      "\u001b[0;34m    some but not all Python code.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    2) Use `tf.py_function`, which allows you to write arbitrary Python code but\u001b[0m\n",
      "\u001b[0;34m    will generally result in worse performance than 1). For example:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> d = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\u001b[0m\n",
      "\u001b[0;34m    >>> # transform a string tensor to upper case string using a Python function\u001b[0m\n",
      "\u001b[0;34m    >>> def upper_case_fn(t: tf.Tensor):\u001b[0m\n",
      "\u001b[0;34m    ...   return t.numpy().decode('utf-8').upper()\u001b[0m\n",
      "\u001b[0;34m    >>> d = d.map(lambda x: tf.py_function(func=upper_case_fn,\u001b[0m\n",
      "\u001b[0;34m    ...           inp=[x], Tout=tf.string))\u001b[0m\n",
      "\u001b[0;34m    >>> list(d.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [b'HELLO', b'WORLD']\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    3) Use `tf.numpy_function`, which also allows you to write arbitrary\u001b[0m\n",
      "\u001b[0;34m    Python code. Note that `tf.py_function` accepts `tf.Tensor` whereas\u001b[0m\n",
      "\u001b[0;34m    `tf.numpy_function` accepts numpy arrays and returns only numpy arrays.\u001b[0m\n",
      "\u001b[0;34m    For example:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> d = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\u001b[0m\n",
      "\u001b[0;34m    >>> def upper_case_fn(t: np.ndarray):\u001b[0m\n",
      "\u001b[0;34m    ...   return t.decode('utf-8').upper()\u001b[0m\n",
      "\u001b[0;34m    >>> d = d.map(lambda x: tf.numpy_function(func=upper_case_fn,\u001b[0m\n",
      "\u001b[0;34m    ...           inp=[x], Tout=tf.string))\u001b[0m\n",
      "\u001b[0;34m    >>> list(d.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [b'HELLO', b'WORLD']\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Note that the use of `tf.numpy_function` and `tf.py_function`\u001b[0m\n",
      "\u001b[0;34m    in general precludes the possibility of executing user-defined\u001b[0m\n",
      "\u001b[0;34m    transformations in parallel (because of Python GIL).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Performance can often be improved by setting `num_parallel_calls` so that\u001b[0m\n",
      "\u001b[0;34m    `map` will use multiple threads to process elements. If deterministic order\u001b[0m\n",
      "\u001b[0;34m    isn't required, it can also improve performance to set\u001b[0m\n",
      "\u001b[0;34m    `deterministic=False`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.map(lambda x: x + 1,\u001b[0m\n",
      "\u001b[0;34m    ...     num_parallel_calls=tf.data.AUTOTUNE,\u001b[0m\n",
      "\u001b[0;34m    ...     deterministic=False)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The order of elements yielded by this transformation is deterministic if\u001b[0m\n",
      "\u001b[0;34m    `deterministic=True`. If `map_func` contains stateful operations and\u001b[0m\n",
      "\u001b[0;34m    `num_parallel_calls > 1`, the order in which that state is accessed is\u001b[0m\n",
      "\u001b[0;34m    undefined, so the values of output elements may not be deterministic\u001b[0m\n",
      "\u001b[0;34m    regardless of the `deterministic` flag value.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      map_func: A function mapping a dataset element to another dataset element.\u001b[0m\n",
      "\u001b[0;34m      num_parallel_calls: (Optional.) A `tf.int64` scalar `tf.Tensor`,\u001b[0m\n",
      "\u001b[0;34m        representing the number elements to process asynchronously in parallel.\u001b[0m\n",
      "\u001b[0;34m        If not specified, elements will be processed sequentially. If the value\u001b[0m\n",
      "\u001b[0;34m        `tf.data.AUTOTUNE` is used, then the number of parallel\u001b[0m\n",
      "\u001b[0;34m        calls is set dynamically based on available CPU.\u001b[0m\n",
      "\u001b[0;34m      deterministic: (Optional.) When `num_parallel_calls` is specified, if this\u001b[0m\n",
      "\u001b[0;34m        boolean is specified (`True` or `False`), it controls the order in which\u001b[0m\n",
      "\u001b[0;34m        the transformation produces elements. If set to `False`, the\u001b[0m\n",
      "\u001b[0;34m        transformation is allowed to yield elements out of order to trade\u001b[0m\n",
      "\u001b[0;34m        determinism for performance. If not specified, the\u001b[0m\n",
      "\u001b[0;34m        `tf.data.Options.deterministic` option (`True` by default) controls the\u001b[0m\n",
      "\u001b[0;34m        behavior.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Maps `map_func` across this dataset and flattens the result.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The type signature is:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m    def flat_map(\u001b[0m\n",
      "\u001b[0;34m      self: Dataset[T],\u001b[0m\n",
      "\u001b[0;34m      map_func: Callable[[T], Dataset[S]]\u001b[0m\n",
      "\u001b[0;34m    ) -> Dataset[S]\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Use `flat_map` if you want to make sure that the order of your dataset\u001b[0m\n",
      "\u001b[0;34m    stays the same. For example, to flatten a dataset of batches into a\u001b[0m\n",
      "\u001b[0;34m    dataset of their elements:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensor_slices(\u001b[0m\n",
      "\u001b[0;34m    ...     [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.flat_map(tf.data.Dataset.from_tensor_slices)\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [1, 2, 3, 4, 5, 6, 7, 8, 9]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    `tf.data.Dataset.interleave()` is a generalization of `flat_map`, since\u001b[0m\n",
      "\u001b[0;34m    `flat_map` produces the same output as\u001b[0m\n",
      "\u001b[0;34m    `tf.data.Dataset.interleave(cycle_length=1)`\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      map_func: A function mapping a dataset element to a dataset.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops -> flat_map_op ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflat_map_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mflat_map_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mignore_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_warning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Drops elements that cause errors.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensor_slices([1., 2., 0., 4.])\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.map(lambda x: tf.debugging.check_numerics(1. / x, \"\"))\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    Traceback (most recent call last):\u001b[0m\n",
      "\u001b[0;34m    ...\u001b[0m\n",
      "\u001b[0;34m    InvalidArgumentError: ... Tensor had Inf values\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.ignore_errors()\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [1.0, 0.5, 0.25]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      log_warning: (Optional.) A bool indicating whether or not ignored errors\u001b[0m\n",
      "\u001b[0;34m        should be logged to stderr. Defaults to `False`.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A string indicating a name for the `tf.data` operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# ignore_errors_op -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mignore_errors_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mignore_errors_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ignore_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_warning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0minterleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mcycle_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mblock_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Maps `map_func` across this dataset, and interleaves the results.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The type signature is:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m    def interleave(\u001b[0m\n",
      "\u001b[0;34m      self: Dataset[T],\u001b[0m\n",
      "\u001b[0;34m      map_func: Callable[[T], Dataset[S]]\u001b[0m\n",
      "\u001b[0;34m    ) -> Dataset[S]\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    For example, you can use `Dataset.interleave()` to process many input files\u001b[0m\n",
      "\u001b[0;34m    concurrently:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> # Preprocess 4 files concurrently, and interleave blocks of 16 records\u001b[0m\n",
      "\u001b[0;34m    >>> # from each file.\u001b[0m\n",
      "\u001b[0;34m    >>> filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\u001b[0m\n",
      "\u001b[0;34m    ...              \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensor_slices(filenames)\u001b[0m\n",
      "\u001b[0;34m    >>> def parse_fn(filename):\u001b[0m\n",
      "\u001b[0;34m    ...   return tf.data.Dataset.range(10)\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.interleave(lambda x:\u001b[0m\n",
      "\u001b[0;34m    ...     tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\u001b[0m\n",
      "\u001b[0;34m    ...     cycle_length=4, block_length=16)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The `cycle_length` and `block_length` arguments control the order in which\u001b[0m\n",
      "\u001b[0;34m    elements are produced. `cycle_length` controls the number of input elements\u001b[0m\n",
      "\u001b[0;34m    that are processed concurrently. If you set `cycle_length` to 1, this\u001b[0m\n",
      "\u001b[0;34m    transformation will handle one input element at a time, and will produce\u001b[0m\n",
      "\u001b[0;34m    identical results to `tf.data.Dataset.flat_map`. In general,\u001b[0m\n",
      "\u001b[0;34m    this transformation will apply `map_func` to `cycle_length` input elements,\u001b[0m\n",
      "\u001b[0;34m    open iterators on the returned `Dataset` objects, and cycle through them\u001b[0m\n",
      "\u001b[0;34m    producing `block_length` consecutive elements from each iterator, and\u001b[0m\n",
      "\u001b[0;34m    consuming the next input element each time it reaches the end of an\u001b[0m\n",
      "\u001b[0;34m    iterator.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    For example:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\u001b[0m\n",
      "\u001b[0;34m    >>> # NOTE: New lines indicate \"block\" boundaries.\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.interleave(\u001b[0m\n",
      "\u001b[0;34m    ...     lambda x: Dataset.from_tensors(x).repeat(6),\u001b[0m\n",
      "\u001b[0;34m    ...     cycle_length=2, block_length=4)\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [1, 1, 1, 1,\u001b[0m\n",
      "\u001b[0;34m     2, 2, 2, 2,\u001b[0m\n",
      "\u001b[0;34m     1, 1,\u001b[0m\n",
      "\u001b[0;34m     2, 2,\u001b[0m\n",
      "\u001b[0;34m     3, 3, 3, 3,\u001b[0m\n",
      "\u001b[0;34m     4, 4, 4, 4,\u001b[0m\n",
      "\u001b[0;34m     3, 3,\u001b[0m\n",
      "\u001b[0;34m     4, 4,\u001b[0m\n",
      "\u001b[0;34m     5, 5, 5, 5,\u001b[0m\n",
      "\u001b[0;34m     5, 5]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Note: The order of elements yielded by this transformation is\u001b[0m\n",
      "\u001b[0;34m    deterministic, as long as `map_func` is a pure function and\u001b[0m\n",
      "\u001b[0;34m    `deterministic=True`. If `map_func` contains any stateful operations, the\u001b[0m\n",
      "\u001b[0;34m    order in which that state is accessed is undefined.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Performance can often be improved by setting `num_parallel_calls` so that\u001b[0m\n",
      "\u001b[0;34m    `interleave` will use multiple threads to fetch elements. If determinism\u001b[0m\n",
      "\u001b[0;34m    isn't required, it can also improve performance to set\u001b[0m\n",
      "\u001b[0;34m    `deterministic=False`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\u001b[0m\n",
      "\u001b[0;34m    ...              \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensor_slices(filenames)\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.interleave(lambda x: tf.data.TFRecordDataset(x),\u001b[0m\n",
      "\u001b[0;34m    ...     cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE,\u001b[0m\n",
      "\u001b[0;34m    ...     deterministic=False)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      map_func: A function that takes a dataset element and returns a\u001b[0m\n",
      "\u001b[0;34m        `tf.data.Dataset`.\u001b[0m\n",
      "\u001b[0;34m      cycle_length: (Optional.) The number of input elements that will be\u001b[0m\n",
      "\u001b[0;34m        processed concurrently. If not set, the tf.data runtime decides what it\u001b[0m\n",
      "\u001b[0;34m        should be based on available CPU. If `num_parallel_calls` is set to\u001b[0m\n",
      "\u001b[0;34m        `tf.data.AUTOTUNE`, the `cycle_length` argument identifies\u001b[0m\n",
      "\u001b[0;34m        the maximum degree of parallelism.\u001b[0m\n",
      "\u001b[0;34m      block_length: (Optional.) The number of consecutive elements to produce\u001b[0m\n",
      "\u001b[0;34m        from each input element before cycling to another input element. If not\u001b[0m\n",
      "\u001b[0;34m        set, defaults to 1.\u001b[0m\n",
      "\u001b[0;34m      num_parallel_calls: (Optional.) If specified, the implementation creates a\u001b[0m\n",
      "\u001b[0;34m        threadpool, which is used to fetch inputs from cycle elements\u001b[0m\n",
      "\u001b[0;34m        asynchronously and in parallel. The default behavior is to fetch inputs\u001b[0m\n",
      "\u001b[0;34m        from cycle elements synchronously with no parallelism. If the value\u001b[0m\n",
      "\u001b[0;34m        `tf.data.AUTOTUNE` is used, then the number of parallel\u001b[0m\n",
      "\u001b[0;34m        calls is set dynamically based on available CPU.\u001b[0m\n",
      "\u001b[0;34m      deterministic: (Optional.) When `num_parallel_calls` is specified, if this\u001b[0m\n",
      "\u001b[0;34m        boolean is specified (`True` or `False`), it controls the order in which\u001b[0m\n",
      "\u001b[0;34m        the transformation produces elements. If set to `False`, the\u001b[0m\n",
      "\u001b[0;34m        transformation is allowed to yield elements out of order to trade\u001b[0m\n",
      "\u001b[0;34m        determinism for performance. If not specified, the\u001b[0m\n",
      "\u001b[0;34m        `tf.data.Options.deterministic` option (`True` by default) controls the\u001b[0m\n",
      "\u001b[0;34m        behavior.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# dataset_ops -> interleave_op -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterleave_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0minterleave_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                     \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Filters this dataset according to `predicate`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.filter(lambda x: x < 3)\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [1, 2]\u001b[0m\n",
      "\u001b[0;34m    >>> # `tf.math.equal(x, y)` is required for equality comparison\u001b[0m\n",
      "\u001b[0;34m    >>> def filter_fn(x):\u001b[0m\n",
      "\u001b[0;34m    ...   return tf.math.equal(x, 1)\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.filter(filter_fn)\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [1]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      predicate: A function mapping a dataset element to a boolean.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops -> filter_op ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfilter_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mfilter_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformation_func\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Applies a transformation function to this dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    `apply` enables chaining of custom `Dataset` transformations, which are\u001b[0m\n",
      "\u001b[0;34m    represented as functions that take one `Dataset` argument and return a\u001b[0m\n",
      "\u001b[0;34m    transformed `Dataset`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.range(100)\u001b[0m\n",
      "\u001b[0;34m    >>> def dataset_fn(ds):\u001b[0m\n",
      "\u001b[0;34m    ...   return ds.filter(lambda x: x < 5)\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.apply(dataset_fn)\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [0, 1, 2, 3, 4]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      transformation_func: A function that takes one `Dataset` argument and\u001b[0m\n",
      "\u001b[0;34m        returns a `Dataset`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformation_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;34mf\"`transformation_func` must return a `tf.data.Dataset` object. \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;34mf\"Got {type(dataset)}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Returns a dataset of \"windows\".\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Each \"window\" is a dataset that contains a subset of elements of the\u001b[0m\n",
      "\u001b[0;34m    input dataset. These are finite datasets of size `size` (or possibly fewer\u001b[0m\n",
      "\u001b[0;34m    if there are not enough input elements to fill the window and\u001b[0m\n",
      "\u001b[0;34m    `drop_remainder` evaluates to `False`).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    For example:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.range(7).window(3)\u001b[0m\n",
      "\u001b[0;34m    >>> for window in dataset:\u001b[0m\n",
      "\u001b[0;34m    ...   print(window)\u001b[0m\n",
      "\u001b[0;34m    <...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\u001b[0m\n",
      "\u001b[0;34m    <...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\u001b[0m\n",
      "\u001b[0;34m    <...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Since windows are datasets, they can be iterated over:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> for window in dataset:\u001b[0m\n",
      "\u001b[0;34m    ...   print(list(window.as_numpy_iterator()))\u001b[0m\n",
      "\u001b[0;34m    [0, 1, 2]\u001b[0m\n",
      "\u001b[0;34m    [3, 4, 5]\u001b[0m\n",
      "\u001b[0;34m    [6]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    #### Shift\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The `shift` argument determines the number of input elements to shift\u001b[0m\n",
      "\u001b[0;34m    between the start of each window. If windows and elements are both numbered\u001b[0m\n",
      "\u001b[0;34m    starting at 0, the first element in window `k` will be element `k * shift`\u001b[0m\n",
      "\u001b[0;34m    of the input dataset. In particular, the first element of the first window\u001b[0m\n",
      "\u001b[0;34m    will always be the first element of the input dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.range(7).window(3, shift=1,\u001b[0m\n",
      "\u001b[0;34m    ...                                           drop_remainder=True)\u001b[0m\n",
      "\u001b[0;34m    >>> for window in dataset:\u001b[0m\n",
      "\u001b[0;34m    ...   print(list(window.as_numpy_iterator()))\u001b[0m\n",
      "\u001b[0;34m    [0, 1, 2]\u001b[0m\n",
      "\u001b[0;34m    [1, 2, 3]\u001b[0m\n",
      "\u001b[0;34m    [2, 3, 4]\u001b[0m\n",
      "\u001b[0;34m    [3, 4, 5]\u001b[0m\n",
      "\u001b[0;34m    [4, 5, 6]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    #### Stride\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The `stride` argument determines the stride between input elements within a\u001b[0m\n",
      "\u001b[0;34m    window.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.range(7).window(3, shift=1, stride=2,\u001b[0m\n",
      "\u001b[0;34m    ...                                           drop_remainder=True)\u001b[0m\n",
      "\u001b[0;34m    >>> for window in dataset:\u001b[0m\n",
      "\u001b[0;34m    ...   print(list(window.as_numpy_iterator()))\u001b[0m\n",
      "\u001b[0;34m    [0, 2, 4]\u001b[0m\n",
      "\u001b[0;34m    [1, 3, 5]\u001b[0m\n",
      "\u001b[0;34m    [2, 4, 6]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    #### Nested elements\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    When the `window` transformation is applied to a dataset whos elements are\u001b[0m\n",
      "\u001b[0;34m    nested structures, it produces a dataset where the elements have the same\u001b[0m\n",
      "\u001b[0;34m    nested structure but each leaf is replaced by a window. In other words,\u001b[0m\n",
      "\u001b[0;34m    the nesting is applied outside of the windows as opposed inside of them.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The type signature is:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m    def window(\u001b[0m\n",
      "\u001b[0;34m        self: Dataset[Nest[T]], ...\u001b[0m\n",
      "\u001b[0;34m    ) -> Dataset[Nest[Dataset[T]]]\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Applying `window` to a `Dataset` of tuples gives a tuple of windows:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensor_slices(([1, 2, 3, 4, 5],\u001b[0m\n",
      "\u001b[0;34m    ...                                               [6, 7, 8, 9, 10]))\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.window(2)\u001b[0m\n",
      "\u001b[0;34m    >>> windows = next(iter(dataset))\u001b[0m\n",
      "\u001b[0;34m    >>> windows\u001b[0m\n",
      "\u001b[0;34m    (<...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>,\u001b[0m\n",
      "\u001b[0;34m     <...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> def to_numpy(ds):\u001b[0m\n",
      "\u001b[0;34m    ...   return list(ds.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    >>>\u001b[0m\n",
      "\u001b[0;34m    >>> for windows in dataset:\u001b[0m\n",
      "\u001b[0;34m    ...   print(to_numpy(windows[0]), to_numpy(windows[1]))\u001b[0m\n",
      "\u001b[0;34m    [1, 2] [6, 7]\u001b[0m\n",
      "\u001b[0;34m    [3, 4] [8, 9]\u001b[0m\n",
      "\u001b[0;34m    [5] [10]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Applying `window` to a `Dataset` of dictionaries gives a dictionary of\u001b[0m\n",
      "\u001b[0;34m    `Datasets`:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensor_slices({'a': [1, 2, 3],\u001b[0m\n",
      "\u001b[0;34m    ...                                               'b': [4, 5, 6],\u001b[0m\n",
      "\u001b[0;34m    ...                                               'c': [7, 8, 9]})\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.window(2)\u001b[0m\n",
      "\u001b[0;34m    >>> def to_numpy(ds):\u001b[0m\n",
      "\u001b[0;34m    ...   return list(ds.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    >>>\u001b[0m\n",
      "\u001b[0;34m    >>> for windows in dataset:\u001b[0m\n",
      "\u001b[0;34m    ...   print(tf.nest.map_structure(to_numpy, windows))\u001b[0m\n",
      "\u001b[0;34m    {'a': [1, 2], 'b': [4, 5], 'c': [7, 8]}\u001b[0m\n",
      "\u001b[0;34m    {'a': [3], 'b': [6], 'c': [9]}\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    #### Flatten a dataset of windows\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The `Dataset.flat_map` and `Dataset.interleave` methods can be used to\u001b[0m\n",
      "\u001b[0;34m    flatten a dataset of windows into a single dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The argument to `flat_map` is a function that takes an element from the\u001b[0m\n",
      "\u001b[0;34m    dataset and returns a `Dataset`. `flat_map` chains together the resulting\u001b[0m\n",
      "\u001b[0;34m    datasets sequentially.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    For example, to turn each window into a dense tensor:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.range(7).window(3, shift=1,\u001b[0m\n",
      "\u001b[0;34m    ...                                           drop_remainder=True)\u001b[0m\n",
      "\u001b[0;34m    >>> batched = dataset.flat_map(lambda x:x.batch(3))\u001b[0m\n",
      "\u001b[0;34m    >>> for batch in batched:\u001b[0m\n",
      "\u001b[0;34m    ...   print(batch.numpy())\u001b[0m\n",
      "\u001b[0;34m    [0 1 2]\u001b[0m\n",
      "\u001b[0;34m    [1 2 3]\u001b[0m\n",
      "\u001b[0;34m    [2 3 4]\u001b[0m\n",
      "\u001b[0;34m    [3 4 5]\u001b[0m\n",
      "\u001b[0;34m    [4 5 6]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      size: A `tf.int64` scalar `tf.Tensor`, representing the number of elements\u001b[0m\n",
      "\u001b[0;34m        of the input dataset to combine into a window. Must be positive.\u001b[0m\n",
      "\u001b[0;34m      shift: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\u001b[0m\n",
      "\u001b[0;34m        number of input elements by which the window moves in each iteration.\u001b[0m\n",
      "\u001b[0;34m        Defaults to `size`. Must be positive.\u001b[0m\n",
      "\u001b[0;34m      stride: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\u001b[0m\n",
      "\u001b[0;34m        stride of the input elements in the sliding window. Must be positive.\u001b[0m\n",
      "\u001b[0;34m        The default value of 1 means \"retain every input element\".\u001b[0m\n",
      "\u001b[0;34m      drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\u001b[0m\n",
      "\u001b[0;34m        whether the last windows should be dropped if their size is smaller than\u001b[0m\n",
      "\u001b[0;34m        `size`.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops -> window_op ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwindow_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mwindow_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Reduces the input dataset to a single element.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The transformation calls `reduce_func` successively on every element of\u001b[0m\n",
      "\u001b[0;34m    the input dataset until the dataset is exhausted, aggregating information in\u001b[0m\n",
      "\u001b[0;34m    its internal state. The `initial_state` argument is used for the initial\u001b[0m\n",
      "\u001b[0;34m    state and the final state is returned as the result.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, _: x + 1).numpy()\u001b[0m\n",
      "\u001b[0;34m    5\u001b[0m\n",
      "\u001b[0;34m    >>> tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + y).numpy()\u001b[0m\n",
      "\u001b[0;34m    10\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      initial_state: An element representing the initial state of the\u001b[0m\n",
      "\u001b[0;34m        transformation.\u001b[0m\n",
      "\u001b[0;34m      reduce_func: A function that maps `(old_state, input_element)` to\u001b[0m\n",
      "\u001b[0;34m        `new_state`. It must take two arguments and return a new element\u001b[0m\n",
      "\u001b[0;34m        The structure of `new_state` must match the structure of\u001b[0m\n",
      "\u001b[0;34m        `initial_state`.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A dataset element corresponding to the final state of the transformation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"initial_state\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstate_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Iteratively rerun the reduce function until reaching a fixed point on\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# `state_structure`.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mneed_to_rerun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mwhile\u001b[0m \u001b[0mneed_to_rerun\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mwrapped_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructured_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructuredFunctionWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mreduce_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;34m\"reduce()\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0minput_structure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0madd_to_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;31m# Extract and validate class information from the returned values.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0moutput_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_classes\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mstate_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;32mlambda\u001b[0m \u001b[0mcomponent_spec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcomponent_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_legacy_output_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mstate_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mfor\u001b[0m \u001b[0mnew_state_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_class\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m              \u001b[0;34mf\"The element classes for the new state must match the initial \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m              \u001b[0;34mf\"state. Expected {state_classes} but got \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m              \u001b[0;34mf\"{wrapped_func.output_classes}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;31m# Extract and validate type information from the returned values.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0moutput_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mstate_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;32mlambda\u001b[0m \u001b[0mcomponent_spec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcomponent_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_legacy_output_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mstate_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mfor\u001b[0m \u001b[0mnew_state_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mnew_state_type\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mstate_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m              \u001b[0;34mf\"The element types for the new state must match the initial \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m              \u001b[0;34mf\"state. Expected {state_types} but got \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m              \u001b[0;34mf\"{wrapped_func.output_types}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;31m# Extract shape information from the returned values.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0moutput_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shapes\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mstate_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;32mlambda\u001b[0m \u001b[0mcomponent_spec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcomponent_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_legacy_output_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mstate_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mflat_state_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mflat_new_state_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mweakened_state_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0moriginal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;32mfor\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_state_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_new_state_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mneed_to_rerun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mfor\u001b[0m \u001b[0moriginal_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweakened_shape\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_state_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                                \u001b[0mweakened_state_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0moriginal_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mweakened_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0moriginal_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mweakened_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mneed_to_rerun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mif\u001b[0m \u001b[0mneed_to_rerun\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# TODO(b/110122868): Support a \"most specific compatible structure\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# method for combining structures, to avoid using legacy structures\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# here.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstate_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_legacy_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mstate_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweakened_state_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mstate_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreduce_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreduce_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_debug_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_metadata_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_and_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstate_structure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mreduce_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0moutput_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flat_tensor_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flat_tensor_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mget_single_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Returns the single element of the `dataset`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The function enables you to use a `tf.data.Dataset` in a stateless\u001b[0m\n",
      "\u001b[0;34m    \"tensor-in tensor-out\" expression, without creating an iterator.\u001b[0m\n",
      "\u001b[0;34m    This facilitates the ease of data transformation on tensors using the\u001b[0m\n",
      "\u001b[0;34m    optimized `tf.data.Dataset` abstraction on top of them.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    For example, lets consider a `preprocessing_fn` which would take as an\u001b[0m\n",
      "\u001b[0;34m    input the raw features and returns the processed feature along with\u001b[0m\n",
      "\u001b[0;34m    it's label.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```python\u001b[0m\n",
      "\u001b[0;34m    def preprocessing_fn(raw_feature):\u001b[0m\n",
      "\u001b[0;34m      # ... the raw_feature is preprocessed as per the use-case\u001b[0m\n",
      "\u001b[0;34m      return feature\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    raw_features = ...  # input batch of BATCH_SIZE elements.\u001b[0m\n",
      "\u001b[0;34m    dataset = (tf.data.Dataset.from_tensor_slices(raw_features)\u001b[0m\n",
      "\u001b[0;34m              .map(preprocessing_fn, num_parallel_calls=BATCH_SIZE)\u001b[0m\n",
      "\u001b[0;34m              .batch(BATCH_SIZE))\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    processed_features = dataset.get_single_element()\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    In the above example, the `raw_features` tensor of length=BATCH_SIZE\u001b[0m\n",
      "\u001b[0;34m    was converted to a `tf.data.Dataset`. Next, each of the `raw_feature` was\u001b[0m\n",
      "\u001b[0;34m    mapped using the `preprocessing_fn` and the processed features were\u001b[0m\n",
      "\u001b[0;34m    grouped into a single batch. The final `dataset` contains only one element\u001b[0m\n",
      "\u001b[0;34m    which is a batch of all the processed features.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    NOTE: The `dataset` should contain only one element.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Now, instead of creating an iterator for the `dataset` and retrieving the\u001b[0m\n",
      "\u001b[0;34m    batch of features, the `tf.data.get_single_element()` function is used\u001b[0m\n",
      "\u001b[0;34m    to skip the iterator creation process and directly output the batch of\u001b[0m\n",
      "\u001b[0;34m    features.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This can be particularly useful when your tensor transformations are\u001b[0m\n",
      "\u001b[0;34m    expressed as `tf.data.Dataset` operations, and you want to use those\u001b[0m\n",
      "\u001b[0;34m    transformations while serving your model.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    #### Keras\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```python\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    model = ... # A pre-built or custom model\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    class PreprocessingModel(tf.keras.Model):\u001b[0m\n",
      "\u001b[0;34m      def __init__(self, model):\u001b[0m\n",
      "\u001b[0;34m        super().__init__(self)\u001b[0m\n",
      "\u001b[0;34m        self.model = model\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m      @tf.function(input_signature=[...])\u001b[0m\n",
      "\u001b[0;34m      def serving_fn(self, data):\u001b[0m\n",
      "\u001b[0;34m        ds = tf.data.Dataset.from_tensor_slices(data)\u001b[0m\n",
      "\u001b[0;34m        ds = ds.map(preprocessing_fn, num_parallel_calls=BATCH_SIZE)\u001b[0m\n",
      "\u001b[0;34m        ds = ds.batch(batch_size=BATCH_SIZE)\u001b[0m\n",
      "\u001b[0;34m        return tf.argmax(self.model(ds.get_single_element()), axis=-1)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    preprocessing_model = PreprocessingModel(model)\u001b[0m\n",
      "\u001b[0;34m    your_exported_model_dir = ... # save the model to this path.\u001b[0m\n",
      "\u001b[0;34m    tf.saved_model.save(preprocessing_model, your_exported_model_dir,\u001b[0m\n",
      "\u001b[0;34m                  signatures={'serving_default': preprocessing_model.serving_fn}\u001b[0m\n",
      "\u001b[0;34m                  )\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A nested structure of `tf.Tensor` objects, corresponding to the single\u001b[0m\n",
      "\u001b[0;34m      element of `dataset`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Raises:\u001b[0m\n",
      "\u001b[0;34m      InvalidArgumentError: (at runtime) if `dataset` does not contain exactly\u001b[0m\n",
      "\u001b[0;34m        one element.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_metadata_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_and_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_to_single_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0munbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Splits elements of a dataset into multiple elements.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    For example, if elements of the dataset are shaped `[B, a0, a1, ...]`,\u001b[0m\n",
      "\u001b[0;34m    where `B` may vary for each input element, then for each element in the\u001b[0m\n",
      "\u001b[0;34m    dataset, the unbatched dataset will contain `B` consecutive elements\u001b[0m\n",
      "\u001b[0;34m    of shape `[a0, a1, ...]`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> elements = [ [1, 2, 3], [1, 2], [1, 2, 3, 4] ]\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_generator(lambda: elements, tf.int64)\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.unbatch()\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [1, 2, 3, 1, 2, 1, 2, 3, 4]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Note: `unbatch` requires a data copy to slice up the batched tensor into\u001b[0m\n",
      "\u001b[0;34m    smaller, unbatched tensors. When optimizing performance, try to avoid\u001b[0m\n",
      "\u001b[0;34m    unnecessary usage of `unbatch`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# dataset_ops -> unbatch_op -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munbatch_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0munbatch_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mwith_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Returns a new `tf.data.Dataset` with the given options set.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The options are \"global\" in the sense they apply to the entire dataset.\u001b[0m\n",
      "\u001b[0;34m    If options are set multiple times, they are merged as long as different\u001b[0m\n",
      "\u001b[0;34m    options do not use different non-default values.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> ds = tf.data.Dataset.range(5)\u001b[0m\n",
      "\u001b[0;34m    >>> ds = ds.interleave(lambda x: tf.data.Dataset.range(5),\u001b[0m\n",
      "\u001b[0;34m    ...                    cycle_length=3,\u001b[0m\n",
      "\u001b[0;34m    ...                    num_parallel_calls=3)\u001b[0m\n",
      "\u001b[0;34m    >>> options = tf.data.Options()\u001b[0m\n",
      "\u001b[0;34m    >>> # This will make the interleave order non-deterministic.\u001b[0m\n",
      "\u001b[0;34m    >>> options.deterministic = False\u001b[0m\n",
      "\u001b[0;34m    >>> ds = ds.with_options(options)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      options: A `tf.data.Options` that identifies the options the use.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Raises:\u001b[0m\n",
      "\u001b[0;34m      ValueError: when an option is set more than once to a non-default value\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0m_OptionsDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mcardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Returns the cardinality of the dataset, if known.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    `cardinality` may return `tf.data.INFINITE_CARDINALITY` if the dataset\u001b[0m\n",
      "\u001b[0;34m    contains an infinite number of elements or `tf.data.UNKNOWN_CARDINALITY` if\u001b[0m\n",
      "\u001b[0;34m    the analysis fails to determine the number of elements in the dataset\u001b[0m\n",
      "\u001b[0;34m    (e.g. when the dataset source is a file).\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.range(42)\u001b[0m\n",
      "\u001b[0;34m    >>> print(dataset.cardinality().numpy())\u001b[0m\n",
      "\u001b[0;34m    42\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.repeat()\u001b[0m\n",
      "\u001b[0;34m    >>> cardinality = dataset.cardinality()\u001b[0m\n",
      "\u001b[0;34m    >>> print((cardinality == tf.data.INFINITE_CARDINALITY).numpy())\u001b[0m\n",
      "\u001b[0;34m    True\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.filter(lambda x: True)\u001b[0m\n",
      "\u001b[0;34m    >>> cardinality = dataset.cardinality()\u001b[0m\n",
      "\u001b[0;34m    >>> print((cardinality == tf.data.UNKNOWN_CARDINALITY).numpy())\u001b[0m\n",
      "\u001b[0;34m    True\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A scalar `tf.int64` `Tensor` representing the cardinality of the dataset.\u001b[0m\n",
      "\u001b[0;34m      If the cardinality is infinite or unknown, `cardinality` returns the\u001b[0m\n",
      "\u001b[0;34m      named constants `tf.data.INFINITE_CARDINALITY` and\u001b[0m\n",
      "\u001b[0;34m      `tf.data.UNKNOWN_CARDINALITY` respectively.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mgroup_by_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mkey_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mreduce_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mwindow_size_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Groups windows of elements by key and reduces them.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This transformation maps each consecutive element in a dataset to a key\u001b[0m\n",
      "\u001b[0;34m    using `key_func` and groups the elements by key. It then applies\u001b[0m\n",
      "\u001b[0;34m    `reduce_func` to at most `window_size_func(key)` elements matching the same\u001b[0m\n",
      "\u001b[0;34m    key. All except the final window for each key will contain\u001b[0m\n",
      "\u001b[0;34m    `window_size_func(key)` elements; the final window may be smaller.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    You may provide either a constant `window_size` or a window size determined\u001b[0m\n",
      "\u001b[0;34m    by the key through `window_size_func`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.range(10)\u001b[0m\n",
      "\u001b[0;34m    >>> window_size = 5\u001b[0m\n",
      "\u001b[0;34m    >>> key_func = lambda x: x%2\u001b[0m\n",
      "\u001b[0;34m    >>> reduce_func = lambda key, dataset: dataset.batch(window_size)\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.group_by_window(\u001b[0m\n",
      "\u001b[0;34m    ...           key_func=key_func,\u001b[0m\n",
      "\u001b[0;34m    ...           reduce_func=reduce_func,\u001b[0m\n",
      "\u001b[0;34m    ...           window_size=window_size)\u001b[0m\n",
      "\u001b[0;34m    >>> for elem in dataset.as_numpy_iterator():\u001b[0m\n",
      "\u001b[0;34m    ...   print(elem)\u001b[0m\n",
      "\u001b[0;34m    [0 2 4 6 8]\u001b[0m\n",
      "\u001b[0;34m    [1 3 5 7 9]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      key_func: A function mapping a nested structure of tensors (having shapes\u001b[0m\n",
      "\u001b[0;34m        and types defined by `self.output_shapes` and `self.output_types`) to a\u001b[0m\n",
      "\u001b[0;34m        scalar `tf.int64` tensor.\u001b[0m\n",
      "\u001b[0;34m      reduce_func: A function mapping a key and a dataset of up to `window_size`\u001b[0m\n",
      "\u001b[0;34m        consecutive elements matching that key to another dataset.\u001b[0m\n",
      "\u001b[0;34m      window_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\u001b[0m\n",
      "\u001b[0;34m        consecutive elements matching the same key to combine in a single batch,\u001b[0m\n",
      "\u001b[0;34m        which will be passed to `reduce_func`. Mutually exclusive with\u001b[0m\n",
      "\u001b[0;34m        `window_size_func`.\u001b[0m\n",
      "\u001b[0;34m      window_size_func: A function mapping a key to a `tf.int64` scalar\u001b[0m\n",
      "\u001b[0;34m        `tf.Tensor`, representing the number of consecutive elements matching\u001b[0m\n",
      "\u001b[0;34m        the same key to combine in a single batch, which will be passed to\u001b[0m\n",
      "\u001b[0;34m        `reduce_func`. Mutually exclusive with `window_size`.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Raises:\u001b[0m\n",
      "\u001b[0;34m      ValueError: if neither or both of {`window_size`, `window_size_func`} are\u001b[0m\n",
      "\u001b[0;34m        passed.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# dataset_ops -> group_by_window_op -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgroup_by_window_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mgroup_by_window_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group_by_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mbucket_by_sequence_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0melement_length_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mbucket_boundaries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mbucket_batch_sizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mpadded_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mpadding_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mpad_to_bucket_boundary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mno_padding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"A transformation that buckets elements in a `Dataset` by length.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Elements of the `Dataset` are grouped together by length and then are padded\u001b[0m\n",
      "\u001b[0;34m    and batched.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This is useful for sequence tasks in which the elements have variable\u001b[0m\n",
      "\u001b[0;34m    length. Grouping together elements that have similar lengths reduces the\u001b[0m\n",
      "\u001b[0;34m    total fraction of padding in a batch which increases training step\u001b[0m\n",
      "\u001b[0;34m    efficiency.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Below is an example to bucketize the input data to the 3 buckets\u001b[0m\n",
      "\u001b[0;34m    \"[0, 3), [3, 5), [5, inf)\" based on sequence length, with batch size 2.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> elements = [\u001b[0m\n",
      "\u001b[0;34m    ...   [0], [1, 2, 3, 4], [5, 6, 7],\u001b[0m\n",
      "\u001b[0;34m    ...   [7, 8, 9, 10, 11], [13, 14, 15, 16, 19, 20], [21, 22]]\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_generator(\u001b[0m\n",
      "\u001b[0;34m    ...     lambda: elements, tf.int64, output_shapes=[None])\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.bucket_by_sequence_length(\u001b[0m\n",
      "\u001b[0;34m    ...         element_length_func=lambda elem: tf.shape(elem)[0],\u001b[0m\n",
      "\u001b[0;34m    ...         bucket_boundaries=[3, 5],\u001b[0m\n",
      "\u001b[0;34m    ...         bucket_batch_sizes=[2, 2, 2])\u001b[0m\n",
      "\u001b[0;34m    >>> for elem in dataset.as_numpy_iterator():\u001b[0m\n",
      "\u001b[0;34m    ...   print(elem)\u001b[0m\n",
      "\u001b[0;34m    [[1 2 3 4]\u001b[0m\n",
      "\u001b[0;34m    [5 6 7 0]]\u001b[0m\n",
      "\u001b[0;34m    [[ 7  8  9 10 11  0]\u001b[0m\n",
      "\u001b[0;34m    [13 14 15 16 19 20]]\u001b[0m\n",
      "\u001b[0;34m    [[ 0  0]\u001b[0m\n",
      "\u001b[0;34m    [21 22]]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      element_length_func: function from element in `Dataset` to `tf.int32`,\u001b[0m\n",
      "\u001b[0;34m        determines the length of the element, which will determine the bucket it\u001b[0m\n",
      "\u001b[0;34m        goes into.\u001b[0m\n",
      "\u001b[0;34m      bucket_boundaries: `list<int>`, upper length boundaries of the buckets.\u001b[0m\n",
      "\u001b[0;34m      bucket_batch_sizes: `list<int>`, batch size per bucket. Length should be\u001b[0m\n",
      "\u001b[0;34m        `len(bucket_boundaries) + 1`.\u001b[0m\n",
      "\u001b[0;34m      padded_shapes: Nested structure of `tf.TensorShape` to pass to\u001b[0m\n",
      "\u001b[0;34m        `tf.data.Dataset.padded_batch`. If not provided, will use\u001b[0m\n",
      "\u001b[0;34m        `dataset.output_shapes`, which will result in variable length dimensions\u001b[0m\n",
      "\u001b[0;34m        being padded out to the maximum length in each batch.\u001b[0m\n",
      "\u001b[0;34m      padding_values: Values to pad with, passed to\u001b[0m\n",
      "\u001b[0;34m        `tf.data.Dataset.padded_batch`. Defaults to padding with 0.\u001b[0m\n",
      "\u001b[0;34m      pad_to_bucket_boundary: bool, if `False`, will pad dimensions with unknown\u001b[0m\n",
      "\u001b[0;34m        size to maximum length in batch. If `True`, will pad dimensions with\u001b[0m\n",
      "\u001b[0;34m        unknown size to bucket boundary minus 1 (i.e., the maximum length in\u001b[0m\n",
      "\u001b[0;34m        each bucket), and caller must ensure that the source `Dataset` does not\u001b[0m\n",
      "\u001b[0;34m        contain any elements with length longer than `max(bucket_boundaries)`.\u001b[0m\n",
      "\u001b[0;34m      no_padding: `bool`, indicates whether to pad the batch features (features\u001b[0m\n",
      "\u001b[0;34m        need to be either of type `tf.sparse.SparseTensor` or of same shape).\u001b[0m\n",
      "\u001b[0;34m      drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\u001b[0m\n",
      "\u001b[0;34m        whether the last batch should be dropped in the case it has fewer than\u001b[0m\n",
      "\u001b[0;34m        `batch_size` elements; the default behavior is not to drop the smaller\u001b[0m\n",
      "\u001b[0;34m        batch.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Raises:\u001b[0m\n",
      "\u001b[0;34m      ValueError: if `len(bucket_batch_sizes) != len(bucket_boundaries) + 1`.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_batch_sizes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_boundaries\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;34mf\"`len(bucket_batch_sizes)` must equal `len(bucket_boundaries) + 1` \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;34mf\"but `len(bucket_batch_sizes)={len(bucket_batch_sizes)}` and \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;34mf\"`len(bucket_boundaries)={len(bucket_boundaries)}`.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_batch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0melement_to_bucket_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;34m\"\"\"Return int64 id of the length bucket for this element.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melement_length_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mboundaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mbuckets_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mboundaries\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mbuckets_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboundaries\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mconditions_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mless_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuckets_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mless\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mbucket_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_min\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconditions_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0mbucket_id\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mwindow_size_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;31m# The window size is set to the batch size for this bucket\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mwindow_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mmake_padded_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnone_filler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mfor\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mnone_filler\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimension_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpadded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mbatching_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrouped_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;34m\"\"\"Batch elements in dataset.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow_size_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mif\u001b[0m \u001b[0mno_padding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mgrouped_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_remainder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mnone_filler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mif\u001b[0m \u001b[0mpad_to_bucket_boundary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"When pad_to_bucket_boundary=True, elements must have \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                   \u001b[0;34m\"length < max(bucket_boundaries).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_less\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mbucket_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_batch_sizes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mboundaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m              \u001b[0mbucket_boundaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mbucket_boundary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboundaries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mnone_filler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucket_boundary\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0minput_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_legacy_output_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrouped_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_padded_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mpadded_shapes\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnone_filler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnone_filler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0mgrouped_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadded_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mshapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mpadding_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_remainder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_by_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mkey_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melement_to_bucket_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mreduce_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatching_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mwindow_size_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_size_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrerandomize_each_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Creates a `Dataset` of pseudorandom values.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The dataset generates a sequence of uniformly distributed integer values.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    `rerandomize_each_iteration` controls whether the sequence of random number\u001b[0m\n",
      "\u001b[0;34m    generated should be re-randomized for each epoch. The default value is False\u001b[0m\n",
      "\u001b[0;34m    where the dataset generates the same sequence of random numbers for each\u001b[0m\n",
      "\u001b[0;34m    epoch.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> ds1 = tf.data.Dataset.random(seed=4).take(10)\u001b[0m\n",
      "\u001b[0;34m    >>> ds2 = tf.data.Dataset.random(seed=4).take(10)\u001b[0m\n",
      "\u001b[0;34m    >>> print(list(ds1.as_numpy_iterator())==list(ds2.as_numpy_iterator()))\u001b[0m\n",
      "\u001b[0;34m    True\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> ds3 = tf.data.Dataset.random(seed=4).take(10)\u001b[0m\n",
      "\u001b[0;34m    >>> ds3_first_epoch = list(ds3.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    >>> ds3_second_epoch = list(ds3.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    >>> print(ds3_first_epoch == ds3_second_epoch)\u001b[0m\n",
      "\u001b[0;34m    True\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> ds4 = tf.data.Dataset.random(\u001b[0m\n",
      "\u001b[0;34m    ...     seed=4, rerandomize_each_iteration=True).take(10)\u001b[0m\n",
      "\u001b[0;34m    >>> ds4_first_epoch = list(ds4.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    >>> ds4_second_epoch = list(ds4.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    >>> print(ds4_first_epoch == ds4_second_epoch)\u001b[0m\n",
      "\u001b[0;34m    False\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      seed: (Optional) If specified, the dataset produces a deterministic\u001b[0m\n",
      "\u001b[0;34m        sequence of values.\u001b[0m\n",
      "\u001b[0;34m      rerandomize_each_iteration: (Optional) If set to False, the dataset\u001b[0m\n",
      "\u001b[0;34m      generates the same sequence of random numbers for each epoch. If set to\u001b[0m\n",
      "\u001b[0;34m      True, it generates a different deterministic sequence of random numbers\u001b[0m\n",
      "\u001b[0;34m      for each epoch. It is defaulted to False if left unspecified.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      Dataset: A `Dataset`.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# dataset_ops -> random_op -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mrandom_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mrerandomize_each_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrerandomize_each_iteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0msnapshot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"AUTO\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mreader_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mshard_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"API to persist the output of the input dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The snapshot API allows users to transparently persist the output of their\u001b[0m\n",
      "\u001b[0;34m    preprocessing pipeline to disk, and materialize the pre-processed data on a\u001b[0m\n",
      "\u001b[0;34m    different training run.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This API enables repeated preprocessing steps to be consolidated, and allows\u001b[0m\n",
      "\u001b[0;34m    re-use of already processed data, trading off disk storage and network\u001b[0m\n",
      "\u001b[0;34m    bandwidth for freeing up more valuable CPU resources and accelerator compute\u001b[0m\n",
      "\u001b[0;34m    time.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    https://github.com/tensorflow/community/blob/master/rfcs/20200107-tf-data-snapshot.md\u001b[0m\n",
      "\u001b[0;34m    has detailed design documentation of this feature.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Users can specify various options to control the behavior of snapshot,\u001b[0m\n",
      "\u001b[0;34m    including how snapshots are read from and written to by passing in\u001b[0m\n",
      "\u001b[0;34m    user-defined functions to the `reader_func` and `shard_func` parameters.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    `shard_func` is a user specified function that maps input elements to\u001b[0m\n",
      "\u001b[0;34m    snapshot shards.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Users may want to specify this function to control how snapshot files should\u001b[0m\n",
      "\u001b[0;34m    be written to disk. Below is an example of how a potential `shard_func`\u001b[0m\n",
      "\u001b[0;34m    could be written.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```python\u001b[0m\n",
      "\u001b[0;34m    dataset = ...\u001b[0m\n",
      "\u001b[0;34m    dataset = dataset.enumerate()\u001b[0m\n",
      "\u001b[0;34m    dataset = dataset.snapshot(\"/path/to/snapshot/dir\",\u001b[0m\n",
      "\u001b[0;34m        shard_func=lambda x, y: x % NUM_SHARDS, ...)\u001b[0m\n",
      "\u001b[0;34m    dataset = dataset.map(lambda x, y: y)\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    `reader_func` is a user specified function that accepts a single argument:\u001b[0m\n",
      "\u001b[0;34m    (1) a Dataset of Datasets, each representing a \"split\" of elements of the\u001b[0m\n",
      "\u001b[0;34m    original dataset. The cardinality of the input dataset matches the\u001b[0m\n",
      "\u001b[0;34m    number of the shards specified in the `shard_func` (see above). The function\u001b[0m\n",
      "\u001b[0;34m    should return a Dataset of elements of the original dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Users may want specify this function to control how snapshot files should be\u001b[0m\n",
      "\u001b[0;34m    read from disk, including the amount of shuffling and parallelism.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Here is an example of a standard reader function a user can define. This\u001b[0m\n",
      "\u001b[0;34m    function enables both dataset shuffling and parallel reading of datasets:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```python\u001b[0m\n",
      "\u001b[0;34m    def user_reader_func(datasets):\u001b[0m\n",
      "\u001b[0;34m      # shuffle the datasets splits\u001b[0m\n",
      "\u001b[0;34m      datasets = datasets.shuffle(NUM_CORES)\u001b[0m\n",
      "\u001b[0;34m      # read datasets in parallel and interleave their elements\u001b[0m\n",
      "\u001b[0;34m      return datasets.interleave(lambda x: x, num_parallel_calls=AUTOTUNE)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    dataset = dataset.snapshot(\"/path/to/snapshot/dir\",\u001b[0m\n",
      "\u001b[0;34m        reader_func=user_reader_func)\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    By default, snapshot parallelizes reads by the number of cores available on\u001b[0m\n",
      "\u001b[0;34m    the system, but will not attempt to shuffle the data.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      path: Required. A directory to use for storing / loading the snapshot to /\u001b[0m\n",
      "\u001b[0;34m        from.\u001b[0m\n",
      "\u001b[0;34m      compression: Optional. The type of compression to apply to the snapshot\u001b[0m\n",
      "\u001b[0;34m        written to disk. Supported options are `GZIP`, `SNAPPY`, `AUTO` or None.\u001b[0m\n",
      "\u001b[0;34m        Defaults to `AUTO`, which attempts to pick an appropriate compression\u001b[0m\n",
      "\u001b[0;34m        algorithm for the dataset.\u001b[0m\n",
      "\u001b[0;34m      reader_func: Optional. A function to control how to read data from\u001b[0m\n",
      "\u001b[0;34m        snapshot shards.\u001b[0m\n",
      "\u001b[0;34m      shard_func: Optional. A function to control how to shard data when writing\u001b[0m\n",
      "\u001b[0;34m        a snapshot.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# dataset_ops -> snapshot_op -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msnapshot_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0msnapshot_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_snapshot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mscan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscan_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"A transformation that scans a function across an input dataset.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    This transformation is a stateful relative of `tf.data.Dataset.map`.\u001b[0m\n",
      "\u001b[0;34m    In addition to mapping `scan_func` across the elements of the input dataset,\u001b[0m\n",
      "\u001b[0;34m    `scan()` accumulates one or more state tensors, whose initial values are\u001b[0m\n",
      "\u001b[0;34m    `initial_state`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.range(10)\u001b[0m\n",
      "\u001b[0;34m    >>> initial_state = tf.constant(0, dtype=tf.int64)\u001b[0m\n",
      "\u001b[0;34m    >>> scan_func = lambda state, i: (state + i, state + i)\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.scan(initial_state=initial_state, scan_func=scan_func)\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [0, 1, 3, 6, 10, 15, 21, 28, 36, 45]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      initial_state: A nested structure of tensors, representing the initial\u001b[0m\n",
      "\u001b[0;34m        state of the accumulator.\u001b[0m\n",
      "\u001b[0;34m      scan_func: A function that maps `(old_state, input_element)` to\u001b[0m\n",
      "\u001b[0;34m        `(new_state, output_element)`. It must take two arguments and return a\u001b[0m\n",
      "\u001b[0;34m        pair of nested structures of tensors. The `new_state` must match the\u001b[0m\n",
      "\u001b[0;34m        structure of `initial_state`.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# scan_op -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscan_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mscan_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscan_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mtake_while\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"A transformation that stops dataset iteration based on a `predicate`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.range(10)\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.take_while(lambda x: x < 5)\u001b[0m\n",
      "\u001b[0;34m    >>> list(dataset.as_numpy_iterator())\u001b[0m\n",
      "\u001b[0;34m    [0, 1, 2, 3, 4]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      predicate: A function that maps a nested structure of tensors (having\u001b[0m\n",
      "\u001b[0;34m        shapes and types defined by `self.output_shapes` and\u001b[0m\n",
      "\u001b[0;34m        `self.output_types`) to a scalar `tf.bool` tensor.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# dataset_ops -> take_while_op -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtake_while_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mtake_while_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_while\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"A transformation that discards duplicate elements of a `Dataset`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Use this transformation to produce a dataset that contains one instance of\u001b[0m\n",
      "\u001b[0;34m    each unique element in the input. For example:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensor_slices([1, 37, 2, 37, 2, 1])\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.unique()\u001b[0m\n",
      "\u001b[0;34m    >>> sorted(list(dataset.as_numpy_iterator()))\u001b[0m\n",
      "\u001b[0;34m    [1, 2, 37]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Note: This transformation only supports datasets which fit into memory\u001b[0m\n",
      "\u001b[0;34m    and have elements of either `tf.int32`, `tf.int64` or `tf.string` type.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency (dataset_ops -> unique_op ->\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munique_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0munique_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mrejection_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Resamples elements to reach a target distribution.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Note: This implementation can reject **or repeat** elements in order to\u001b[0m\n",
      "\u001b[0;34m    reach the `target_dist`. So, in some cases, the output `Dataset` may be\u001b[0m\n",
      "\u001b[0;34m    larger than the input `Dataset`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> initial_dist = [0.6, 0.4]\u001b[0m\n",
      "\u001b[0;34m    >>> n = 1000\u001b[0m\n",
      "\u001b[0;34m    >>> elems = np.random.choice(len(initial_dist), size=n, p=initial_dist)\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = tf.data.Dataset.from_tensor_slices(elems)\u001b[0m\n",
      "\u001b[0;34m    >>> zero, one = np.bincount(list(dataset.as_numpy_iterator())) / n\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Following from `initial_dist`, `zero` is ~0.6 and `one` is ~0.4.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    >>> target_dist = [0.5, 0.5]\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.rejection_resample(\u001b[0m\n",
      "\u001b[0;34m    ...    class_func=lambda x: x,\u001b[0m\n",
      "\u001b[0;34m    ...    target_dist=target_dist,\u001b[0m\n",
      "\u001b[0;34m    ...    initial_dist=initial_dist)\u001b[0m\n",
      "\u001b[0;34m    >>> dataset = dataset.map(lambda class_func_result, data: data)\u001b[0m\n",
      "\u001b[0;34m    >>> zero, one = np.bincount(list(dataset.as_numpy_iterator())) / n\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Following from `target_dist`, `zero` is ~0.5 and `one` is ~0.5.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      class_func: A function mapping an element of the input dataset to a scalar\u001b[0m\n",
      "\u001b[0;34m        `tf.int32` tensor. Values should be in `[0, num_classes)`.\u001b[0m\n",
      "\u001b[0;34m      target_dist: A floating point type tensor, shaped `[num_classes]`.\u001b[0m\n",
      "\u001b[0;34m      initial_dist: (Optional.)  A floating point type tensor, shaped\u001b[0m\n",
      "\u001b[0;34m        `[num_classes]`.  If not provided, the true class distribution is\u001b[0m\n",
      "\u001b[0;34m        estimated live in a streaming fashion.\u001b[0m\n",
      "\u001b[0;34m      seed: (Optional.) Python integer seed for the resampler.\u001b[0m\n",
      "\u001b[0;34m      name: (Optional.) A name for the tf.data operation.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# TODO(b/245793127): Consider switching back to the 'v1' implementation.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtarget_dist_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"target_dist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtarget_dist_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dist_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Get initial distribution.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0minitial_dist\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0minitial_dist_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"initial_dist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0minitial_dist_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_dist_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0macceptance_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_of_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0m_calculate_acceptance_probs_with_mixing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_dist_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                                  \u001b[0mtarget_dist_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0minitial_dist_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0minitial_dist_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0macceptance_dist_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0macceptance_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mprob_of_original_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mprob_of_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0minitial_dist_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_estimate_initial_dist_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mtarget_dist_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0macceptance_and_original_prob_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_dist_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;32mlambda\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_calculate_acceptance_probs_with_mixing\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# pylint: disable=g-long-lambda\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m              \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_dist_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0macceptance_dist_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macceptance_and_original_prob_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;32mlambda\u001b[0m \u001b[0maccept_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maccept_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mprob_of_original_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macceptance_and_original_prob_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_original\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprob_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfiltered_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_filter_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macceptance_dist_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_dist_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                             \u001b[0mclass_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Prefetch filtered dataset for speed.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfiltered_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprob_original_static\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_prob_original_static\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0minitial_dist_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_dist_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minitial_dist\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0madd_class_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mclass_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mclass_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mprob_original_static\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_class_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32melif\u001b[0m \u001b[0mprob_original_static\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0mfiltered_ds\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_from_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_class_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_ds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob_of_original_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m          \u001b[0mstop_on_empty_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0msample_from_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mstop_on_empty_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mrerandomize_each_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Samples elements at random from the datasets in `datasets`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Creates a dataset by interleaving elements of `datasets` with `weight[i]`\u001b[0m\n",
      "\u001b[0;34m    probability of picking an element from dataset `i`. Sampling is done without\u001b[0m\n",
      "\u001b[0;34m    replacement. For example, suppose we have 2 datasets:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```python\u001b[0m\n",
      "\u001b[0;34m    dataset1 = tf.data.Dataset.range(0, 3)\u001b[0m\n",
      "\u001b[0;34m    dataset2 = tf.data.Dataset.range(100, 103)\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Suppose that we sample from these 2 datasets with the following weights:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```python\u001b[0m\n",
      "\u001b[0;34m    sample_dataset = tf.data.Dataset.sample_from_datasets(\u001b[0m\n",
      "\u001b[0;34m        [dataset1, dataset2], weights=[0.5, 0.5])\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    One possible outcome of elements in sample_dataset is:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m    print(list(sample_dataset.as_numpy_iterator()))\u001b[0m\n",
      "\u001b[0;34m    # [100, 0, 1, 101, 2, 102]\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      datasets: A non-empty list of `tf.data.Dataset` objects with compatible\u001b[0m\n",
      "\u001b[0;34m        structure.\u001b[0m\n",
      "\u001b[0;34m      weights: (Optional.) A list or Tensor of `len(datasets)` floating-point\u001b[0m\n",
      "\u001b[0;34m        values where `weights[i]` represents the probability to sample from\u001b[0m\n",
      "\u001b[0;34m        `datasets[i]`, or a `tf.data.Dataset` object where each element is such\u001b[0m\n",
      "\u001b[0;34m        a list. Defaults to a uniform distribution across `datasets`.\u001b[0m\n",
      "\u001b[0;34m      seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the random\u001b[0m\n",
      "\u001b[0;34m        seed that will be used to create the distribution. See\u001b[0m\n",
      "\u001b[0;34m        `tf.random.set_seed` for behavior.\u001b[0m\n",
      "\u001b[0;34m      stop_on_empty_dataset: If `True`, sampling stops if it encounters an empty\u001b[0m\n",
      "\u001b[0;34m        dataset. If `False`, it continues sampling and skips any empty datasets.\u001b[0m\n",
      "\u001b[0;34m        It is recommended to set it to `True`. Otherwise, the distribution of\u001b[0m\n",
      "\u001b[0;34m        samples starts off as the user intends, but may change as input datasets\u001b[0m\n",
      "\u001b[0;34m        become empty. This can be difficult to detect since the dataset starts\u001b[0m\n",
      "\u001b[0;34m        off looking correct. Default to `False` for backward compatibility.\u001b[0m\n",
      "\u001b[0;34m      rerandomize_each_iteration: An optional `bool`. The boolean argument\u001b[0m\n",
      "\u001b[0;34m      controls whether the sequence of random numbers used to determine which\u001b[0m\n",
      "\u001b[0;34m      dataset to sample from will be rerandomized each epoch. That is, it\u001b[0m\n",
      "\u001b[0;34m      determinies whether datasets will be sampled in the same order across\u001b[0m\n",
      "\u001b[0;34m      different epochs (the default behavior) or not.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A dataset that interleaves elements from `datasets` at random, according\u001b[0m\n",
      "\u001b[0;34m      to `weights` if provided, otherwise with uniform probability.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Raises:\u001b[0m\n",
      "\u001b[0;34m      TypeError: If the `datasets` or `weights` arguments have the wrong type.\u001b[0m\n",
      "\u001b[0;34m      ValueError:\u001b[0m\n",
      "\u001b[0;34m        - If `datasets` is empty, or\u001b[0m\n",
      "\u001b[0;34m        - If `weights` is specified and does not match the length of `datasets`.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# (dataset_ops -> sample_from_datasets_op -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msample_from_datasets_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0msample_from_datasets_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_from_datasets\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstop_on_empty_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mrerandomize_each_iteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;32mdef\u001b[0m \u001b[0mchoose_from_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m      \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchoice_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_on_empty_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m  \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DatasetV2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Creates a dataset that deterministically chooses elements from `datasets`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    For example, given the following datasets:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```python\u001b[0m\n",
      "\u001b[0;34m    datasets = [tf.data.Dataset.from_tensors(\"foo\").repeat(),\u001b[0m\n",
      "\u001b[0;34m                tf.data.Dataset.from_tensors(\"bar\").repeat(),\u001b[0m\n",
      "\u001b[0;34m                tf.data.Dataset.from_tensors(\"baz\").repeat()]\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    # Define a dataset containing `[0, 1, 2, 0, 1, 2, 0, 1, 2]`.\u001b[0m\n",
      "\u001b[0;34m    choice_dataset = tf.data.Dataset.range(3).repeat(3)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    result = tf.data.Dataset.choose_from_datasets(datasets, choice_dataset)\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    The elements of `result` will be:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m    \"foo\", \"bar\", \"baz\", \"foo\", \"bar\", \"baz\", \"foo\", \"bar\", \"baz\"\u001b[0m\n",
      "\u001b[0;34m    ```\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Args:\u001b[0m\n",
      "\u001b[0;34m      datasets: A non-empty list of `tf.data.Dataset` objects with compatible\u001b[0m\n",
      "\u001b[0;34m        structure.\u001b[0m\n",
      "\u001b[0;34m      choice_dataset: A `tf.data.Dataset` of scalar `tf.int64` tensors between\u001b[0m\n",
      "\u001b[0;34m        `0` and `len(datasets) - 1`.\u001b[0m\n",
      "\u001b[0;34m      stop_on_empty_dataset: If `True`, selection stops if it encounters an\u001b[0m\n",
      "\u001b[0;34m        empty dataset. If `False`, it skips empty datasets. It is recommended to\u001b[0m\n",
      "\u001b[0;34m        set it to `True`. Otherwise, the selected elements start off as the user\u001b[0m\n",
      "\u001b[0;34m        intends, but may change as input datasets become empty. This can be\u001b[0m\n",
      "\u001b[0;34m        difficult to detect since the dataset starts off looking correct.\u001b[0m\n",
      "\u001b[0;34m        Defaults to `True`.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Returns:\u001b[0m\n",
      "\u001b[0;34m      A new `Dataset` with the transformation applied as described above.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    Raises:\u001b[0m\n",
      "\u001b[0;34m      TypeError: If `datasets` or `choice_dataset` has the wrong type.\u001b[0m\n",
      "\u001b[0;34m      ValueError: If `datasets` is empty.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# Loaded lazily due to a circular dependency\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# (dataset_ops -> choose_from_datasets_op -> dataset_ops).\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mchoose_from_datasets_op\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mchoose_from_datasets_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_choose_from_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchoice_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_on_empty_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m           /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py\n",
      "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
      "\u001b[0;31mSubclasses:\u001b[0m     DatasetV1, DatasetSource, UnaryDataset, _VariantDataset, TFRecordDatasetV2, _PerDeviceGenerator, _ReincarnatedPerDeviceGenerator"
     ]
    }
   ],
   "source": [
    "tf.data.Dataset??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 188 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "data = tf.keras.utils.image_dataset_from_directory('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_iterator.next()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
